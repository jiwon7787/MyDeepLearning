{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "os.chdir(r\"C:\\Users\\jiwon\\OneDrive\\Desktop\\deep-learning-from-scratch-master\\ch04\")\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"認識率99%以上の高精度なConvNet\n",
    "\n",
    "    ネットワーク構成は下記の通り\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 重みの初期化===========\n",
    "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
    "\n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        # 畳み込み層の重みとバイアスを設定\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        # 全結合層の重みとバイアスを設定\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'],\n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'],\n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'],\n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    # 推論ではモデルの全力を見るためdropoutは使わないからデフォルトでtrain_flg=False\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            # layerがDropoutクラスのインスタンスであるかどうかをチェック\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    # 学習時の勾配計算(gradient, numerical_gradient関数)では過学習抑制のためにデフォルトでドロップアウト使えるようにする)\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size] # [0:100], [101:200]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)): #畳み込みとAffineがある層\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 1 train loss:1.1862023481083912\n",
      "No. 2 train loss:0.8872046709815676\n",
      "No. 3 train loss:0.9629477021893981\n",
      "No. 4 train loss:1.087585705050825\n",
      "No. 5 train loss:0.962573601076834\n",
      "No. 6 train loss:1.1178823753817597\n",
      "No. 7 train loss:1.0554104272698945\n",
      "No. 8 train loss:0.9926777308362934\n",
      "No. 9 train loss:0.9067889449155889\n",
      "No. 10 train loss:0.9438404809967623\n",
      "No. 11 train loss:1.0049082783472376\n",
      "No. 12 train loss:1.0404236652749634\n",
      "No. 13 train loss:1.0647668475307548\n",
      "No. 14 train loss:1.0157067419456667\n",
      "No. 15 train loss:1.0199662265582292\n",
      "No. 16 train loss:0.8440529328709\n",
      "No. 17 train loss:1.0650880535069895\n",
      "No. 18 train loss:1.2230110049612535\n",
      "No. 19 train loss:1.1281290477359187\n",
      "No. 20 train loss:0.945986863429403\n",
      "No. 21 train loss:1.0978023670500814\n",
      "No. 22 train loss:0.9033732976166178\n",
      "No. 23 train loss:1.047005310288722\n",
      "No. 24 train loss:0.9347681664962708\n",
      "No. 25 train loss:1.0799752268179683\n",
      "No. 26 train loss:1.066154786947889\n",
      "No. 27 train loss:1.036627736316964\n",
      "No. 28 train loss:1.0673013012025476\n",
      "No. 29 train loss:1.023323523097705\n",
      "No. 30 train loss:1.0006007311288527\n",
      "No. 31 train loss:0.8950233516377459\n",
      "No. 32 train loss:0.909542269898075\n",
      "No. 33 train loss:1.1430888792731657\n",
      "No. 34 train loss:1.0684290315931022\n",
      "No. 35 train loss:1.0006875806161868\n",
      "No. 36 train loss:0.9666488001811229\n",
      "No. 37 train loss:1.0412720096349641\n",
      "No. 38 train loss:0.9407264448156638\n",
      "No. 39 train loss:0.9526988430520557\n",
      "No. 40 train loss:1.0986493049969681\n",
      "No. 41 train loss:0.9750898675026549\n",
      "No. 42 train loss:1.0115991316863453\n",
      "No. 43 train loss:1.0942864860720423\n",
      "No. 44 train loss:1.2460902519240777\n",
      "No. 45 train loss:0.9412271548685045\n",
      "No. 46 train loss:1.0399505604959165\n",
      "No. 47 train loss:0.937181793754357\n",
      "No. 48 train loss:1.049929574483944\n",
      "No. 49 train loss:0.9712879659044144\n",
      "No. 50 train loss:1.0725173040095413\n",
      "=== epoch:1, train acc:0.98, test acc:0.98 ===\n",
      "No. 51 train loss:1.0070654742109462\n",
      "No. 52 train loss:0.9737742893639535\n",
      "No. 53 train loss:1.1435897450153412\n",
      "No. 54 train loss:0.9802193914992988\n",
      "No. 55 train loss:0.9206594581877695\n",
      "No. 56 train loss:1.005891386371628\n",
      "No. 57 train loss:0.9544038456551103\n",
      "No. 58 train loss:0.9842567449788078\n",
      "No. 59 train loss:0.9939167320639095\n",
      "No. 60 train loss:1.0238733923086396\n",
      "No. 61 train loss:0.8722932757801474\n",
      "No. 62 train loss:1.0621275385012199\n",
      "No. 63 train loss:1.2133883194703252\n",
      "No. 64 train loss:0.7677983534062649\n",
      "No. 65 train loss:1.2508033649635266\n",
      "No. 66 train loss:1.0359354032207753\n",
      "No. 67 train loss:1.0683367118303508\n",
      "No. 68 train loss:0.8928855947390375\n",
      "No. 69 train loss:1.110948010410309\n",
      "No. 70 train loss:1.0297981668996505\n",
      "No. 71 train loss:1.0038381491702548\n",
      "No. 72 train loss:1.0921368657677468\n",
      "No. 73 train loss:0.938398166145926\n",
      "No. 74 train loss:0.9752720486443752\n",
      "No. 75 train loss:1.0785750596447528\n",
      "No. 76 train loss:1.1634638412985965\n",
      "No. 77 train loss:0.9478669326654864\n",
      "No. 78 train loss:0.9814380581052998\n",
      "No. 79 train loss:0.9138304301882588\n",
      "No. 80 train loss:1.1726220152366493\n",
      "No. 81 train loss:1.1055041834471322\n",
      "No. 82 train loss:1.0463701764494546\n",
      "No. 83 train loss:0.831884136175196\n",
      "No. 84 train loss:1.0208599290765712\n",
      "No. 85 train loss:1.1310711093971957\n",
      "No. 86 train loss:0.9743583842578037\n",
      "No. 87 train loss:0.962429091747783\n",
      "No. 88 train loss:0.9556956476982483\n",
      "No. 89 train loss:0.9993199864603808\n",
      "No. 90 train loss:0.9882282177683375\n",
      "No. 91 train loss:0.9858870984139341\n",
      "No. 92 train loss:1.0262632298816756\n",
      "No. 93 train loss:1.0010742075733463\n",
      "No. 94 train loss:1.056085228429202\n",
      "No. 95 train loss:1.0142727213516418\n",
      "No. 96 train loss:1.0285060171205722\n",
      "No. 97 train loss:1.0406055693684015\n",
      "No. 98 train loss:0.8923059030481452\n",
      "No. 99 train loss:0.860993865461663\n",
      "No. 100 train loss:0.97354001085703\n",
      "=== epoch:2, train acc:0.986, test acc:0.977 ===\n",
      "No. 101 train loss:1.0053264291605375\n",
      "No. 102 train loss:1.1396011003871334\n",
      "No. 103 train loss:0.9834703452771757\n",
      "No. 104 train loss:0.9318111596815882\n",
      "No. 105 train loss:0.9413057030667461\n",
      "No. 106 train loss:1.0554268340695707\n",
      "No. 107 train loss:1.0131626733264683\n",
      "No. 108 train loss:0.8379324069811015\n",
      "No. 109 train loss:1.0024650795876608\n",
      "No. 110 train loss:1.0414196552954262\n",
      "No. 111 train loss:0.8549076689329409\n",
      "No. 112 train loss:1.04814587449585\n",
      "No. 113 train loss:1.0126415841925824\n",
      "No. 114 train loss:1.0459211968162816\n",
      "No. 115 train loss:0.8757430337338259\n",
      "No. 116 train loss:1.0733561413977553\n",
      "No. 117 train loss:0.9238957745544415\n",
      "No. 118 train loss:1.2207352441291979\n",
      "No. 119 train loss:1.0155764981663058\n",
      "No. 120 train loss:0.8724316643835962\n",
      "No. 121 train loss:1.044145485526278\n",
      "No. 122 train loss:1.1277407865526385\n",
      "No. 123 train loss:0.9945400016526368\n",
      "No. 124 train loss:1.0874146147683275\n",
      "No. 125 train loss:0.9647990083279878\n",
      "No. 126 train loss:1.0619480402328016\n",
      "No. 127 train loss:0.9967797852014122\n",
      "No. 128 train loss:0.9166850205345515\n",
      "No. 129 train loss:1.0379082759864828\n",
      "No. 130 train loss:1.0303062061944142\n",
      "No. 131 train loss:1.0245461290727687\n",
      "No. 132 train loss:1.0612486759846693\n",
      "No. 133 train loss:0.8276042337458398\n",
      "No. 134 train loss:0.9795384975073721\n",
      "No. 135 train loss:1.1234608505713495\n",
      "No. 136 train loss:0.9500405401982295\n",
      "No. 137 train loss:0.9665354375079782\n",
      "No. 138 train loss:1.0272391784058987\n",
      "No. 139 train loss:0.9237039229837107\n",
      "No. 140 train loss:0.8723479236328983\n",
      "No. 141 train loss:0.8854760188575805\n",
      "No. 142 train loss:1.0273640253033702\n",
      "No. 143 train loss:0.8105669514097006\n",
      "No. 144 train loss:0.974954084865524\n",
      "No. 145 train loss:0.9500990567635204\n",
      "No. 146 train loss:1.0246100368191786\n",
      "No. 147 train loss:0.9336580277367327\n",
      "No. 148 train loss:0.8003329233672347\n",
      "No. 149 train loss:0.9211909001779145\n",
      "No. 150 train loss:1.059136333811595\n",
      "=== epoch:3, train acc:0.987, test acc:0.974 ===\n",
      "No. 151 train loss:0.9504668443682133\n",
      "No. 152 train loss:1.0501786928493566\n",
      "No. 153 train loss:0.8990430776084607\n",
      "No. 154 train loss:0.9133852290017291\n",
      "No. 155 train loss:1.0956536039559728\n",
      "No. 156 train loss:0.9759094838245774\n",
      "No. 157 train loss:0.9151620115391377\n",
      "No. 158 train loss:0.9481740806834796\n",
      "No. 159 train loss:1.0602494804496914\n",
      "No. 160 train loss:1.1148480185027447\n",
      "No. 161 train loss:1.0524044611781498\n",
      "No. 162 train loss:0.9984700419001002\n",
      "No. 163 train loss:0.988021863358157\n",
      "No. 164 train loss:0.9996048823246434\n",
      "No. 165 train loss:0.829709397758037\n",
      "No. 166 train loss:1.0878200732490146\n",
      "No. 167 train loss:0.8455985332676196\n",
      "No. 168 train loss:1.1307643900965436\n",
      "No. 169 train loss:0.975915507650971\n",
      "No. 170 train loss:1.0959988875080178\n",
      "No. 171 train loss:1.0485221745961344\n",
      "No. 172 train loss:0.9552514311793071\n",
      "No. 173 train loss:1.0258353323014717\n",
      "No. 174 train loss:0.8344435985968589\n",
      "No. 175 train loss:0.8545904497252366\n",
      "No. 176 train loss:0.9658022193724425\n",
      "No. 177 train loss:0.8863539378367921\n",
      "No. 178 train loss:0.8749920591745035\n",
      "No. 179 train loss:0.8488106589307836\n",
      "No. 180 train loss:0.8361812773247997\n",
      "No. 181 train loss:1.019726780654051\n",
      "No. 182 train loss:1.02757189407593\n",
      "No. 183 train loss:1.0149199056098817\n",
      "No. 184 train loss:1.0019461606072673\n",
      "No. 185 train loss:1.0026429300263653\n",
      "No. 186 train loss:0.9971370601351182\n",
      "No. 187 train loss:1.0301332869041888\n",
      "No. 188 train loss:0.8987944639264601\n",
      "No. 189 train loss:0.9957934160018254\n",
      "No. 190 train loss:1.0729470973492545\n",
      "No. 191 train loss:1.0302570412240704\n",
      "No. 192 train loss:0.836801283868039\n",
      "No. 193 train loss:1.0598309509795354\n",
      "No. 194 train loss:0.8935601137488547\n",
      "No. 195 train loss:0.8886658426736416\n",
      "No. 196 train loss:0.9749525427667781\n",
      "No. 197 train loss:1.1559164809456663\n",
      "No. 198 train loss:0.9869492755114241\n",
      "No. 199 train loss:1.1010965853440222\n",
      "No. 200 train loss:0.9846137510959727\n",
      "=== epoch:4, train acc:0.988, test acc:0.977 ===\n",
      "No. 201 train loss:0.9445709201543778\n",
      "No. 202 train loss:1.031457921585899\n",
      "No. 203 train loss:1.0080412422952607\n",
      "No. 204 train loss:0.9691444853629736\n",
      "No. 205 train loss:0.8903885643437076\n",
      "No. 206 train loss:0.9551511810081823\n",
      "No. 207 train loss:0.8614734012943811\n",
      "No. 208 train loss:1.0867036452072152\n",
      "No. 209 train loss:0.9761572622959608\n",
      "No. 210 train loss:1.1750480551699372\n",
      "No. 211 train loss:0.8483126884369735\n",
      "No. 212 train loss:0.960351474452977\n",
      "No. 213 train loss:0.9828213524046286\n",
      "No. 214 train loss:1.0232188106440272\n",
      "No. 215 train loss:0.9960148271194297\n",
      "No. 216 train loss:0.9442158497267671\n",
      "No. 217 train loss:1.0687770007618678\n",
      "No. 218 train loss:1.0817770807937062\n",
      "No. 219 train loss:0.9568550838894648\n",
      "No. 220 train loss:0.7321993680312628\n",
      "No. 221 train loss:1.0011214459862625\n",
      "No. 222 train loss:0.9340641955677205\n",
      "No. 223 train loss:0.9381786457184782\n",
      "No. 224 train loss:1.1094921782457332\n",
      "No. 225 train loss:0.7717209602492612\n",
      "No. 226 train loss:1.1561564558527686\n",
      "No. 227 train loss:0.8759095719016396\n",
      "No. 228 train loss:0.7753066535681573\n",
      "No. 229 train loss:0.9660030670591466\n",
      "No. 230 train loss:1.019328309043004\n",
      "No. 231 train loss:1.0434816490270293\n",
      "No. 232 train loss:0.8471529283149141\n",
      "No. 233 train loss:0.7927800601495452\n",
      "No. 234 train loss:0.8374828067165173\n",
      "No. 235 train loss:0.8288623818086707\n",
      "No. 236 train loss:0.8402070911911274\n",
      "No. 237 train loss:0.9544288646869813\n",
      "No. 238 train loss:0.8936606616178746\n",
      "No. 239 train loss:0.9263874954582623\n",
      "No. 240 train loss:0.8343691950732037\n",
      "No. 241 train loss:0.9343459765436868\n",
      "No. 242 train loss:0.9713041096429978\n",
      "No. 243 train loss:1.019901285010699\n",
      "No. 244 train loss:1.1727433378512768\n",
      "No. 245 train loss:0.953012273771579\n",
      "No. 246 train loss:0.9776106252001303\n",
      "No. 247 train loss:0.8723921305267897\n",
      "No. 248 train loss:1.121697425386987\n",
      "No. 249 train loss:1.012784727257639\n",
      "No. 250 train loss:0.9005216180293307\n",
      "=== epoch:5, train acc:0.992, test acc:0.983 ===\n",
      "No. 251 train loss:0.8831565018345242\n",
      "No. 252 train loss:1.0855199015937156\n",
      "No. 253 train loss:0.9273117858670442\n",
      "No. 254 train loss:1.0477982507772277\n",
      "No. 255 train loss:0.9765179149199187\n",
      "No. 256 train loss:1.0476746224141942\n",
      "No. 257 train loss:1.0182229437218764\n",
      "No. 258 train loss:1.1793891685302422\n",
      "No. 259 train loss:1.0236012900828495\n",
      "No. 260 train loss:1.0584621795103224\n",
      "No. 261 train loss:0.9559664550283816\n",
      "No. 262 train loss:0.9306520359211453\n",
      "No. 263 train loss:1.0394675160679907\n",
      "No. 264 train loss:0.8323822724538094\n",
      "No. 265 train loss:1.0065265335767746\n",
      "No. 266 train loss:0.9698222085355479\n",
      "No. 267 train loss:0.8928402327363655\n",
      "No. 268 train loss:0.8475543695410217\n",
      "No. 269 train loss:1.0827560779726244\n",
      "No. 270 train loss:1.0246345022486394\n",
      "No. 271 train loss:0.9313045624333518\n",
      "No. 272 train loss:1.176192002790069\n",
      "No. 273 train loss:1.0556514295068518\n",
      "No. 274 train loss:1.0085921722787037\n",
      "No. 275 train loss:0.8905954458922501\n",
      "No. 276 train loss:0.9361045009513783\n",
      "No. 277 train loss:0.9511569233249415\n",
      "No. 278 train loss:0.9661415929611508\n",
      "No. 279 train loss:0.9996308243232573\n",
      "No. 280 train loss:0.9949131230707173\n",
      "No. 281 train loss:0.9881587525000459\n",
      "No. 282 train loss:0.7801714050307522\n",
      "No. 283 train loss:0.8536160482557087\n",
      "No. 284 train loss:0.8911775767296318\n",
      "No. 285 train loss:0.9196057706075526\n",
      "No. 286 train loss:1.0516666435896482\n",
      "No. 287 train loss:0.8933291648306981\n",
      "No. 288 train loss:0.886400910509413\n",
      "No. 289 train loss:1.038855915377727\n",
      "No. 290 train loss:1.1975735491542527\n",
      "No. 291 train loss:0.969572831846043\n",
      "No. 292 train loss:0.923398101831906\n",
      "No. 293 train loss:1.0509984241618513\n",
      "No. 294 train loss:1.0647348906546292\n",
      "No. 295 train loss:0.9381070924605897\n",
      "No. 296 train loss:0.8805414290915755\n",
      "No. 297 train loss:1.0554461091435003\n",
      "No. 298 train loss:1.0591464140672529\n",
      "No. 299 train loss:0.8051465374815052\n",
      "No. 300 train loss:1.1342983997942186\n",
      "=== epoch:6, train acc:0.991, test acc:0.979 ===\n",
      "No. 301 train loss:0.9268999167327924\n",
      "No. 302 train loss:1.022773541925269\n",
      "No. 303 train loss:0.8812825975242654\n",
      "No. 304 train loss:0.9134749890887874\n",
      "No. 305 train loss:0.9070322092301116\n",
      "No. 306 train loss:0.9097770186153399\n",
      "No. 307 train loss:1.0008280443518982\n",
      "No. 308 train loss:0.9064955383469022\n",
      "No. 309 train loss:1.1442217134741814\n",
      "No. 310 train loss:0.8842114056219893\n",
      "No. 311 train loss:0.9508910841189828\n",
      "No. 312 train loss:0.7323712286099144\n",
      "No. 313 train loss:1.0341967419049491\n",
      "No. 314 train loss:1.017758969060059\n",
      "No. 315 train loss:0.8321065884173935\n",
      "No. 316 train loss:0.9321906787904737\n",
      "No. 317 train loss:0.9852790480363767\n",
      "No. 318 train loss:0.9268250704546095\n",
      "No. 319 train loss:0.9573831692140041\n",
      "No. 320 train loss:0.9436074487339361\n",
      "No. 321 train loss:1.0431268189033256\n",
      "No. 322 train loss:1.1896925512450054\n",
      "No. 323 train loss:0.9536146471333656\n",
      "No. 324 train loss:1.0772916237591017\n",
      "No. 325 train loss:1.0202671494664468\n",
      "No. 326 train loss:0.858689609981828\n",
      "No. 327 train loss:0.9147824834562589\n",
      "No. 328 train loss:0.7450641503956313\n",
      "No. 329 train loss:0.9694774044948905\n",
      "No. 330 train loss:0.8214346949982039\n",
      "No. 331 train loss:0.8491095670109436\n",
      "No. 332 train loss:0.8434062972688934\n",
      "No. 333 train loss:1.0538624179805924\n",
      "No. 334 train loss:0.824623905563829\n",
      "No. 335 train loss:1.0326191442059416\n",
      "No. 336 train loss:0.933804267091189\n",
      "No. 337 train loss:1.1319719713912002\n",
      "No. 338 train loss:1.0726943468172296\n",
      "No. 339 train loss:0.9214342470037975\n",
      "No. 340 train loss:0.872999048332462\n",
      "No. 341 train loss:0.9836508369979504\n",
      "No. 342 train loss:0.8620050473374054\n",
      "No. 343 train loss:1.0582808389212233\n",
      "No. 344 train loss:0.866875235012139\n",
      "No. 345 train loss:0.9276395305637426\n",
      "No. 346 train loss:0.9420052343378578\n",
      "No. 347 train loss:0.902021081079607\n",
      "No. 348 train loss:0.948549833141558\n",
      "No. 349 train loss:1.0738552801913932\n",
      "No. 350 train loss:1.0396058971857092\n",
      "=== epoch:7, train acc:0.998, test acc:0.985 ===\n",
      "No. 351 train loss:1.0326726593953417\n",
      "No. 352 train loss:1.0835041750370211\n",
      "No. 353 train loss:0.9310941378257119\n",
      "No. 354 train loss:0.8705025585458692\n",
      "No. 355 train loss:0.9736248195112391\n",
      "No. 356 train loss:0.9410207302360547\n",
      "No. 357 train loss:0.9456098757059986\n",
      "No. 358 train loss:0.7826167644395926\n",
      "No. 359 train loss:0.9424077737020236\n",
      "No. 360 train loss:0.9415600452256959\n",
      "No. 361 train loss:0.9420177190333645\n",
      "No. 362 train loss:1.144754253570995\n",
      "No. 363 train loss:0.8447547183033071\n",
      "No. 364 train loss:1.0306339906205788\n",
      "No. 365 train loss:0.9363066511994056\n",
      "No. 366 train loss:0.8713693347556942\n",
      "No. 367 train loss:0.9388707704056116\n",
      "No. 368 train loss:0.8957747753163104\n",
      "No. 369 train loss:0.7462141602979986\n",
      "No. 370 train loss:0.9874835432145522\n",
      "No. 371 train loss:0.9173241149301119\n",
      "No. 372 train loss:0.9768837905334334\n",
      "No. 373 train loss:0.848053123862901\n",
      "No. 374 train loss:1.1176477163117595\n",
      "No. 375 train loss:1.0340082722930841\n",
      "No. 376 train loss:0.7884378234253873\n",
      "No. 377 train loss:1.016726147578076\n",
      "No. 378 train loss:1.1326431435407485\n",
      "No. 379 train loss:1.0032799023388885\n",
      "No. 380 train loss:1.030287146163209\n",
      "No. 381 train loss:0.9140678093428272\n",
      "No. 382 train loss:0.883620866060686\n",
      "No. 383 train loss:0.9347012056097124\n",
      "No. 384 train loss:0.9846575812034919\n",
      "No. 385 train loss:0.9108929800072887\n",
      "No. 386 train loss:0.9464103698358782\n",
      "No. 387 train loss:0.7855612861341972\n",
      "No. 388 train loss:0.8595705447033404\n",
      "No. 389 train loss:1.0240522118122177\n",
      "No. 390 train loss:0.952345561034792\n",
      "No. 391 train loss:0.8068324586058618\n",
      "No. 392 train loss:0.9551527026896883\n",
      "No. 393 train loss:0.7881600453419568\n",
      "No. 394 train loss:0.8624497921452481\n",
      "No. 395 train loss:0.8452166666074896\n",
      "No. 396 train loss:0.7607332180031686\n",
      "No. 397 train loss:0.8927487800786241\n",
      "No. 398 train loss:0.8954883216350247\n",
      "No. 399 train loss:1.000944621157532\n",
      "No. 400 train loss:0.9006466298297658\n",
      "=== epoch:8, train acc:0.993, test acc:0.981 ===\n",
      "No. 401 train loss:0.9385248050358719\n",
      "No. 402 train loss:0.9584727026115849\n",
      "No. 403 train loss:0.9679051881668311\n",
      "No. 404 train loss:0.8791926971874302\n",
      "No. 405 train loss:0.7959025016487474\n",
      "No. 406 train loss:0.9873587300328422\n",
      "No. 407 train loss:0.9027942555595381\n",
      "No. 408 train loss:0.8859044037771651\n",
      "No. 409 train loss:0.9496978498179668\n",
      "No. 410 train loss:1.0083158007297246\n",
      "No. 411 train loss:0.8512194189357127\n",
      "No. 412 train loss:0.8641785949994238\n",
      "No. 413 train loss:0.7300548704381196\n",
      "No. 414 train loss:0.8960742307583743\n",
      "No. 415 train loss:0.9429386388851867\n",
      "No. 416 train loss:1.063353898810561\n",
      "No. 417 train loss:0.9005474871182296\n",
      "No. 418 train loss:0.861522610504027\n",
      "No. 419 train loss:0.9388751318056054\n",
      "No. 420 train loss:1.053125159090024\n",
      "No. 421 train loss:1.0034336937452684\n",
      "No. 422 train loss:1.0123787855560529\n",
      "No. 423 train loss:0.9118976709670074\n",
      "No. 424 train loss:0.9184086468865312\n",
      "No. 425 train loss:0.8961550704281247\n",
      "No. 426 train loss:0.7290517922569316\n",
      "No. 427 train loss:0.8019245215986044\n",
      "No. 428 train loss:0.9866306710240681\n",
      "No. 429 train loss:1.0035770630294136\n",
      "No. 430 train loss:0.7896608698089743\n",
      "No. 431 train loss:1.1732735408364228\n",
      "No. 432 train loss:0.9720867445648713\n",
      "No. 433 train loss:0.7915774676661498\n",
      "No. 434 train loss:1.0093345179174764\n",
      "No. 435 train loss:0.8369991089397882\n",
      "No. 436 train loss:0.9357095850167313\n",
      "No. 437 train loss:0.9436049383851173\n",
      "No. 438 train loss:0.8933549887930415\n",
      "No. 439 train loss:1.0113906701341853\n",
      "No. 440 train loss:0.8818707348314828\n",
      "No. 441 train loss:0.9445898616076464\n",
      "No. 442 train loss:0.8798581721729125\n",
      "No. 443 train loss:0.9705940680199105\n",
      "No. 444 train loss:0.8621289680221084\n",
      "No. 445 train loss:1.0658599305327345\n",
      "No. 446 train loss:0.9196597877882543\n",
      "No. 447 train loss:0.9930582629249488\n",
      "No. 448 train loss:0.9274806582153482\n",
      "No. 449 train loss:0.9147479141911825\n",
      "No. 450 train loss:1.0877605549098361\n",
      "=== epoch:9, train acc:0.994, test acc:0.983 ===\n",
      "No. 451 train loss:0.8069972056663154\n",
      "No. 452 train loss:1.081291604344323\n",
      "No. 453 train loss:0.8778725378707939\n",
      "No. 454 train loss:0.8868674155980658\n",
      "No. 455 train loss:1.0502427900320803\n",
      "No. 456 train loss:1.0337107982040563\n",
      "No. 457 train loss:0.9714639941613682\n",
      "No. 458 train loss:1.0162040122183214\n",
      "No. 459 train loss:1.0749971335637023\n",
      "No. 460 train loss:0.9296717253619553\n",
      "No. 461 train loss:0.8987109733259967\n",
      "No. 462 train loss:1.0438047635957062\n",
      "No. 463 train loss:0.8855282250918299\n",
      "No. 464 train loss:0.8593051683485616\n",
      "No. 465 train loss:0.7434076290718553\n",
      "No. 466 train loss:1.0743953303959934\n",
      "No. 467 train loss:0.790632318889996\n",
      "No. 468 train loss:0.932186691376182\n",
      "No. 469 train loss:0.8210942494662545\n",
      "No. 470 train loss:1.1052491995094569\n",
      "No. 471 train loss:1.0204043663412998\n",
      "No. 472 train loss:0.9113013716423142\n",
      "No. 473 train loss:1.0432581164656958\n",
      "No. 474 train loss:1.0210272402028702\n",
      "No. 475 train loss:0.8546398011044916\n",
      "No. 476 train loss:1.0323170769307517\n",
      "No. 477 train loss:1.0120815601559006\n",
      "No. 478 train loss:0.9267925427718733\n",
      "No. 479 train loss:0.9661776345072943\n",
      "No. 480 train loss:0.9467087723119029\n",
      "No. 481 train loss:0.7896182587567568\n",
      "No. 482 train loss:0.916959625361288\n",
      "No. 483 train loss:0.9148621120288583\n",
      "No. 484 train loss:0.8217928064087999\n",
      "No. 485 train loss:1.0006826700047236\n",
      "No. 486 train loss:0.8845413677694005\n",
      "No. 487 train loss:0.9662575172637001\n",
      "No. 488 train loss:0.8703182026476091\n",
      "No. 489 train loss:0.9695084111652484\n",
      "No. 490 train loss:0.8105836381103498\n",
      "No. 491 train loss:0.8026280854311032\n",
      "No. 492 train loss:0.9061978783752745\n",
      "No. 493 train loss:0.829705648606906\n",
      "No. 494 train loss:0.9723002465515376\n",
      "No. 495 train loss:1.1580683730508923\n",
      "No. 496 train loss:0.9303744999884376\n",
      "No. 497 train loss:0.8912606616314471\n",
      "No. 498 train loss:0.9486292211191314\n",
      "No. 499 train loss:0.8620466854527402\n",
      "No. 500 train loss:1.007892266795553\n",
      "=== epoch:10, train acc:0.99, test acc:0.979 ===\n",
      "No. 501 train loss:0.9542648821079506\n",
      "No. 502 train loss:1.0736051525737438\n",
      "No. 503 train loss:0.8325314781164201\n",
      "No. 504 train loss:0.919936968891221\n",
      "No. 505 train loss:0.9074296119398089\n",
      "No. 506 train loss:1.126136280221685\n",
      "No. 507 train loss:0.8478376921152827\n",
      "No. 508 train loss:0.9853138922959235\n",
      "No. 509 train loss:0.9241526248966108\n",
      "No. 510 train loss:0.7754055505078603\n",
      "No. 511 train loss:1.0093932813782263\n",
      "No. 512 train loss:1.0530092310792438\n",
      "No. 513 train loss:0.9579517336908934\n",
      "No. 514 train loss:0.8199529794896725\n",
      "No. 515 train loss:0.9497252139828649\n",
      "No. 516 train loss:0.9059370091974437\n",
      "No. 517 train loss:0.9137593790401689\n",
      "No. 518 train loss:0.9696850394349354\n",
      "No. 519 train loss:0.901839835511188\n",
      "No. 520 train loss:0.9728422240942245\n",
      "No. 521 train loss:1.0776914369764277\n",
      "No. 522 train loss:0.8653474783636029\n",
      "No. 523 train loss:0.841752840285252\n",
      "No. 524 train loss:0.9029146249839779\n",
      "No. 525 train loss:0.9525684862519427\n",
      "No. 526 train loss:0.8919956741453844\n",
      "No. 527 train loss:0.9716182784150601\n",
      "No. 528 train loss:0.9838195948786835\n",
      "No. 529 train loss:0.7722065515839528\n",
      "No. 530 train loss:0.8965584418452589\n",
      "No. 531 train loss:0.8917468256887128\n",
      "No. 532 train loss:1.009094924505258\n",
      "No. 533 train loss:0.9669242077227338\n",
      "No. 534 train loss:0.869254904031973\n",
      "No. 535 train loss:0.9279524116007323\n",
      "No. 536 train loss:0.8548131510029721\n",
      "No. 537 train loss:0.8799300579565354\n",
      "No. 538 train loss:0.9596245888747346\n",
      "No. 539 train loss:1.009710104028194\n",
      "No. 540 train loss:0.9881637055725931\n",
      "No. 541 train loss:0.9397808802104768\n",
      "No. 542 train loss:0.8792598977445286\n",
      "No. 543 train loss:0.786028107777458\n",
      "No. 544 train loss:0.879291650944125\n",
      "No. 545 train loss:0.9507296956256246\n",
      "No. 546 train loss:0.8564785682938744\n",
      "No. 547 train loss:0.8942592006485615\n",
      "No. 548 train loss:0.9233715306445893\n",
      "No. 549 train loss:0.7805444009416711\n",
      "No. 550 train loss:0.9300702632252508\n",
      "=== epoch:11, train acc:0.998, test acc:0.988 ===\n",
      "No. 551 train loss:0.9632370446570329\n",
      "No. 552 train loss:0.7565038322832669\n",
      "No. 553 train loss:0.8539051290311233\n",
      "No. 554 train loss:0.8265372130864165\n",
      "No. 555 train loss:0.9094347439424117\n",
      "No. 556 train loss:0.9081604510885564\n",
      "No. 557 train loss:0.9603637369875355\n",
      "No. 558 train loss:0.7812486356623439\n",
      "No. 559 train loss:0.8715347460577667\n",
      "No. 560 train loss:0.878276611285014\n",
      "No. 561 train loss:0.9685810096098324\n",
      "No. 562 train loss:1.0045812550918478\n",
      "No. 563 train loss:0.816305161781452\n",
      "No. 564 train loss:0.7983852630696502\n",
      "No. 565 train loss:0.9337018036048427\n",
      "No. 566 train loss:0.8524910840483476\n",
      "No. 567 train loss:0.7325188098570534\n",
      "No. 568 train loss:0.8697414473131224\n",
      "No. 569 train loss:1.0028031225499938\n",
      "No. 570 train loss:0.9957933560511547\n",
      "No. 571 train loss:0.7637611694958134\n",
      "No. 572 train loss:0.9896515373924842\n",
      "No. 573 train loss:0.9777858466519177\n",
      "No. 574 train loss:0.8847215489887872\n",
      "No. 575 train loss:0.9523988014505762\n",
      "No. 576 train loss:0.8148778687720086\n",
      "No. 577 train loss:0.9679908147203637\n",
      "No. 578 train loss:1.015959315352483\n",
      "No. 579 train loss:0.9694175574886122\n",
      "No. 580 train loss:1.0616852164822665\n",
      "No. 581 train loss:0.9618507139744689\n",
      "No. 582 train loss:1.170606612634884\n",
      "No. 583 train loss:0.9598062052919606\n",
      "No. 584 train loss:0.8715020698259108\n",
      "No. 585 train loss:0.836396882940847\n",
      "No. 586 train loss:0.9343890090505524\n",
      "No. 587 train loss:0.8201789591912807\n",
      "No. 588 train loss:0.9493894923769913\n",
      "No. 589 train loss:1.0221857722146925\n",
      "No. 590 train loss:0.9427656312860796\n",
      "No. 591 train loss:1.0257007987569018\n",
      "No. 592 train loss:1.0098178838667045\n",
      "No. 593 train loss:1.029238773581983\n",
      "No. 594 train loss:1.0088717794547089\n",
      "No. 595 train loss:0.9397343319466596\n",
      "No. 596 train loss:1.0555095688958176\n",
      "No. 597 train loss:0.8441674484227426\n",
      "No. 598 train loss:1.0232690226021977\n",
      "No. 599 train loss:0.8886377383246378\n",
      "No. 600 train loss:0.9084916608824044\n",
      "=== epoch:12, train acc:0.999, test acc:0.984 ===\n",
      "No. 601 train loss:0.9508447874832714\n",
      "No. 602 train loss:0.8536591941134769\n",
      "No. 603 train loss:1.0180923610843018\n",
      "No. 604 train loss:0.9619547450920541\n",
      "No. 605 train loss:0.9130873214792138\n",
      "No. 606 train loss:0.8072296924935268\n",
      "No. 607 train loss:0.8500028346375522\n",
      "No. 608 train loss:0.834774209252554\n",
      "No. 609 train loss:0.844837455905187\n",
      "No. 610 train loss:1.0263571415499906\n",
      "No. 611 train loss:1.0315017844186132\n",
      "No. 612 train loss:0.8536733299682291\n",
      "No. 613 train loss:1.2117470694121568\n",
      "No. 614 train loss:0.9408337473979099\n",
      "No. 615 train loss:0.9760573298776528\n",
      "No. 616 train loss:0.9162478930905747\n",
      "No. 617 train loss:0.9740204938016269\n",
      "No. 618 train loss:0.8717115656637137\n",
      "No. 619 train loss:0.8254931902059899\n",
      "No. 620 train loss:0.932217929833258\n",
      "No. 621 train loss:0.8039830867164617\n",
      "No. 622 train loss:0.9214372367349614\n",
      "No. 623 train loss:0.8927385529490537\n",
      "No. 624 train loss:0.7720564621862436\n",
      "No. 625 train loss:1.1455794349117976\n",
      "No. 626 train loss:0.7971199379008199\n",
      "No. 627 train loss:0.8752472266231156\n",
      "No. 628 train loss:0.9525531447454846\n",
      "No. 629 train loss:0.8743950954719156\n",
      "No. 630 train loss:0.9622275490903249\n",
      "No. 631 train loss:0.8800458204938911\n",
      "No. 632 train loss:0.9266158110456307\n",
      "No. 633 train loss:0.8879400478010357\n",
      "No. 634 train loss:0.9194871073189809\n",
      "No. 635 train loss:0.9769341826991029\n",
      "No. 636 train loss:1.0657972523704244\n",
      "No. 637 train loss:0.8224142269975546\n",
      "No. 638 train loss:0.8789122874901152\n",
      "No. 639 train loss:1.0204554216141324\n",
      "No. 640 train loss:0.9019997588258759\n",
      "No. 641 train loss:0.9735469972955129\n",
      "No. 642 train loss:0.9486255725025995\n",
      "No. 643 train loss:0.9851278585555889\n",
      "No. 644 train loss:0.8488962036591472\n",
      "No. 645 train loss:0.8515681566496511\n",
      "No. 646 train loss:1.0598118318195033\n",
      "No. 647 train loss:0.9849964949695647\n",
      "No. 648 train loss:0.8522096574411897\n",
      "No. 649 train loss:0.9423305864350763\n",
      "No. 650 train loss:0.9322072448269093\n",
      "=== epoch:13, train acc:0.997, test acc:0.986 ===\n",
      "No. 651 train loss:1.0855499347890827\n",
      "No. 652 train loss:0.910047637767729\n",
      "No. 653 train loss:1.004899886038015\n",
      "No. 654 train loss:0.9213145964685269\n",
      "No. 655 train loss:0.7404126202831799\n",
      "No. 656 train loss:0.8445937034891904\n",
      "No. 657 train loss:0.8663353833969256\n",
      "No. 658 train loss:0.8908844348321792\n",
      "No. 659 train loss:0.8992362799900918\n",
      "No. 660 train loss:0.859963161905542\n",
      "No. 661 train loss:0.7561200536139177\n",
      "No. 662 train loss:1.0485656904075986\n",
      "No. 663 train loss:1.1679096726868523\n",
      "No. 664 train loss:0.9171501826106407\n",
      "No. 665 train loss:0.762890696303128\n",
      "No. 666 train loss:0.8041502741601927\n",
      "No. 667 train loss:0.9439209926772398\n",
      "No. 668 train loss:0.9340812510459477\n",
      "No. 669 train loss:0.8381110924635681\n",
      "No. 670 train loss:0.853698546784572\n",
      "No. 671 train loss:0.8955539219932556\n",
      "No. 672 train loss:0.8597158410203468\n",
      "No. 673 train loss:0.9653002379507882\n",
      "No. 674 train loss:0.9088637531072922\n",
      "No. 675 train loss:0.9959121088807783\n",
      "No. 676 train loss:0.8406417818369104\n",
      "No. 677 train loss:0.9067447580591107\n",
      "No. 678 train loss:0.8246752731734875\n",
      "No. 679 train loss:0.9492535933110058\n",
      "No. 680 train loss:0.8835656505768078\n",
      "No. 681 train loss:1.0135228264294769\n",
      "No. 682 train loss:1.0414798283700006\n",
      "No. 683 train loss:0.8721979188232942\n",
      "No. 684 train loss:0.9130248700915794\n",
      "No. 685 train loss:0.8244127047840729\n",
      "No. 686 train loss:0.8562382012245109\n",
      "No. 687 train loss:0.8818965604582304\n",
      "No. 688 train loss:0.8537828149916947\n",
      "No. 689 train loss:0.9366685882112694\n",
      "No. 690 train loss:1.007698264275402\n",
      "No. 691 train loss:0.7889255391807447\n",
      "No. 692 train loss:1.110248968138747\n",
      "No. 693 train loss:1.058957479511543\n",
      "No. 694 train loss:0.7988014753438338\n",
      "No. 695 train loss:1.034211622098063\n",
      "No. 696 train loss:1.161008599107646\n",
      "No. 697 train loss:0.8460809350975559\n",
      "No. 698 train loss:0.9490173028132743\n",
      "No. 699 train loss:1.0752842046033377\n",
      "No. 700 train loss:0.9093883013327044\n",
      "=== epoch:14, train acc:0.997, test acc:0.984 ===\n",
      "No. 701 train loss:0.843588445282067\n",
      "No. 702 train loss:0.775001314349874\n",
      "No. 703 train loss:1.0472844359769233\n",
      "No. 704 train loss:1.0181182139858913\n",
      "No. 705 train loss:1.0141895661622433\n",
      "No. 706 train loss:0.7589147198106165\n",
      "No. 707 train loss:0.8664058288744009\n",
      "No. 708 train loss:0.8503577156628052\n",
      "No. 709 train loss:0.7911522356308254\n",
      "No. 710 train loss:0.9426449386260671\n",
      "No. 711 train loss:1.1231303855539507\n",
      "No. 712 train loss:0.9482659771447196\n",
      "No. 713 train loss:0.9027797332463721\n",
      "No. 714 train loss:0.819817848329838\n",
      "No. 715 train loss:1.0242559457479523\n",
      "No. 716 train loss:0.7975984110217206\n",
      "No. 717 train loss:0.967868907591956\n",
      "No. 718 train loss:0.8639499094571044\n",
      "No. 719 train loss:0.9451847595423041\n",
      "No. 720 train loss:0.9229029588460294\n",
      "No. 721 train loss:0.8240192840328504\n",
      "No. 722 train loss:0.8334582205075713\n",
      "No. 723 train loss:0.8744682907080253\n",
      "No. 724 train loss:1.1090706593658772\n",
      "No. 725 train loss:0.9078971041060953\n",
      "No. 726 train loss:0.9171129855518885\n",
      "No. 727 train loss:0.8439383596237271\n",
      "No. 728 train loss:0.8092327522712439\n",
      "No. 729 train loss:0.9331854829986825\n",
      "No. 730 train loss:0.8332775960979691\n",
      "No. 731 train loss:0.9349042195691798\n",
      "No. 732 train loss:0.821907386670014\n",
      "No. 733 train loss:0.8298812525668327\n",
      "No. 734 train loss:0.8155138749157461\n",
      "No. 735 train loss:0.937556357415839\n",
      "No. 736 train loss:0.8722302247881291\n",
      "No. 737 train loss:0.8195734788962187\n",
      "No. 738 train loss:0.8724330395620062\n",
      "No. 739 train loss:1.032489396438496\n",
      "No. 740 train loss:0.9657577131697701\n",
      "No. 741 train loss:0.8383710023558766\n",
      "No. 742 train loss:0.8452853293683487\n",
      "No. 743 train loss:0.9658264954471032\n",
      "No. 744 train loss:0.9664540149221859\n",
      "No. 745 train loss:0.9868695030392164\n",
      "No. 746 train loss:0.968767323551907\n",
      "No. 747 train loss:0.8392422318083983\n",
      "No. 748 train loss:1.0845582341708602\n",
      "No. 749 train loss:0.8893404708863919\n",
      "No. 750 train loss:0.8032062039713582\n",
      "=== epoch:15, train acc:0.995, test acc:0.985 ===\n",
      "No. 751 train loss:1.0895874204576343\n",
      "No. 752 train loss:0.9051882316034403\n",
      "No. 753 train loss:0.9206308812274441\n",
      "No. 754 train loss:0.8607916281906592\n",
      "No. 755 train loss:0.6918025223396309\n",
      "No. 756 train loss:0.9587308501929445\n",
      "No. 757 train loss:0.7172592687117992\n",
      "No. 758 train loss:0.9455332503517634\n",
      "No. 759 train loss:0.8350304102144281\n",
      "No. 760 train loss:0.8639189632796003\n",
      "No. 761 train loss:0.932926177520339\n",
      "No. 762 train loss:0.8401358547437031\n",
      "No. 763 train loss:0.9929974026791198\n",
      "No. 764 train loss:0.9722549282405586\n",
      "No. 765 train loss:0.8419780381032331\n",
      "No. 766 train loss:0.859394632725794\n",
      "No. 767 train loss:0.9988991549794516\n",
      "No. 768 train loss:0.9321021702403257\n",
      "No. 769 train loss:0.8331748556054311\n",
      "No. 770 train loss:0.7644126368471629\n",
      "No. 771 train loss:0.8862383912211859\n",
      "No. 772 train loss:0.8358956414774305\n",
      "No. 773 train loss:0.9274540676530932\n",
      "No. 774 train loss:1.0203749584361592\n",
      "No. 775 train loss:0.8500120869230394\n",
      "No. 776 train loss:0.7857447694686289\n",
      "No. 777 train loss:0.7955148780430267\n",
      "No. 778 train loss:0.9221829082984181\n",
      "No. 779 train loss:0.6554590746556195\n",
      "No. 780 train loss:0.9179189507848307\n",
      "No. 781 train loss:0.9055993640820053\n",
      "No. 782 train loss:0.8822358428142009\n",
      "No. 783 train loss:0.8152494819680076\n",
      "No. 784 train loss:0.8304541891098037\n",
      "No. 785 train loss:0.857099437963711\n",
      "No. 786 train loss:0.9666574996753272\n",
      "No. 787 train loss:0.9186167220715568\n",
      "No. 788 train loss:0.9332315776594787\n",
      "No. 789 train loss:0.9389884807755499\n",
      "No. 790 train loss:1.0145924392328065\n",
      "No. 791 train loss:0.8118389684950954\n",
      "No. 792 train loss:0.7245180531442117\n",
      "No. 793 train loss:1.0377731803384929\n",
      "No. 794 train loss:0.825788632186693\n",
      "No. 795 train loss:0.7844691149020853\n",
      "No. 796 train loss:0.8706722099230808\n",
      "No. 797 train loss:0.9477822870216258\n",
      "No. 798 train loss:0.7750332766225517\n",
      "No. 799 train loss:1.0303029572505686\n",
      "No. 800 train loss:0.9339610722001724\n",
      "=== epoch:16, train acc:0.993, test acc:0.985 ===\n",
      "No. 801 train loss:0.9435558078093322\n",
      "No. 802 train loss:0.8857986459323282\n",
      "No. 803 train loss:0.8871443574095775\n",
      "No. 804 train loss:0.8193870862895314\n",
      "No. 805 train loss:0.8640324807035813\n",
      "No. 806 train loss:0.7914731309246011\n",
      "No. 807 train loss:0.7559532042545035\n",
      "No. 808 train loss:0.9861131625110013\n",
      "No. 809 train loss:0.9049923868498965\n",
      "No. 810 train loss:0.8331659027091938\n",
      "No. 811 train loss:1.1473635584148907\n",
      "No. 812 train loss:0.9459198928397857\n",
      "No. 813 train loss:0.7871320759552645\n",
      "No. 814 train loss:0.8408282100271306\n",
      "No. 815 train loss:0.8313596478305122\n",
      "No. 816 train loss:0.9827398549317056\n",
      "No. 817 train loss:0.8682281027835757\n",
      "No. 818 train loss:0.7850359575361746\n",
      "No. 819 train loss:0.9408900786097665\n",
      "No. 820 train loss:0.9318107240542848\n",
      "No. 821 train loss:0.9898889512262641\n",
      "No. 822 train loss:0.8126325096090649\n",
      "No. 823 train loss:0.9483086344817253\n",
      "No. 824 train loss:1.0476538992518232\n",
      "No. 825 train loss:0.9610657682117523\n",
      "No. 826 train loss:0.9255050077511555\n",
      "No. 827 train loss:0.8839605905687751\n",
      "No. 828 train loss:1.0311480280237422\n",
      "No. 829 train loss:0.8576587888966474\n",
      "No. 830 train loss:1.0704098754710014\n",
      "No. 831 train loss:0.9366180321348692\n",
      "No. 832 train loss:0.9208203346245076\n",
      "No. 833 train loss:0.8253369877898752\n",
      "No. 834 train loss:0.7862367109192402\n",
      "No. 835 train loss:0.9706700652911979\n",
      "No. 836 train loss:1.0021327840963128\n",
      "No. 837 train loss:0.893408875739385\n",
      "No. 838 train loss:0.9937398832224404\n",
      "No. 839 train loss:0.8994856345291758\n",
      "No. 840 train loss:0.9037001250032699\n",
      "No. 841 train loss:0.972399023793446\n",
      "No. 842 train loss:0.9047394195181211\n",
      "No. 843 train loss:0.7929097730588249\n",
      "No. 844 train loss:0.8828712732166282\n",
      "No. 845 train loss:0.8115956306617739\n",
      "No. 846 train loss:0.8881293046030989\n",
      "No. 847 train loss:0.8236229179589035\n",
      "No. 848 train loss:0.6711002284383101\n",
      "No. 849 train loss:0.9495340971607671\n",
      "No. 850 train loss:0.774094430448138\n",
      "=== epoch:17, train acc:0.997, test acc:0.981 ===\n",
      "No. 851 train loss:0.9743883063226731\n",
      "No. 852 train loss:1.006648078734909\n",
      "No. 853 train loss:0.9303620169519803\n",
      "No. 854 train loss:0.7909495788567453\n",
      "No. 855 train loss:0.8804024771683959\n",
      "No. 856 train loss:0.9344479318792872\n",
      "No. 857 train loss:0.8546437836789561\n",
      "No. 858 train loss:0.8898012719967714\n",
      "No. 859 train loss:0.8469123267774086\n",
      "No. 860 train loss:0.8093313911315666\n",
      "No. 861 train loss:0.9901785424978894\n",
      "No. 862 train loss:1.0203118718413753\n",
      "No. 863 train loss:0.8885010410164863\n",
      "No. 864 train loss:0.9235804421968381\n",
      "No. 865 train loss:0.9105415751930382\n",
      "No. 866 train loss:0.728141729356235\n",
      "No. 867 train loss:0.7847935100882069\n",
      "No. 868 train loss:0.8953550138961366\n",
      "No. 869 train loss:0.9107963292761085\n",
      "No. 870 train loss:0.8497755511420487\n",
      "No. 871 train loss:0.8341893134545173\n",
      "No. 872 train loss:0.9168428025455873\n",
      "No. 873 train loss:0.7707016334556736\n",
      "No. 874 train loss:0.991817349895589\n",
      "No. 875 train loss:0.9878174157267707\n",
      "No. 876 train loss:0.891562142621866\n",
      "No. 877 train loss:0.9317103249957033\n",
      "No. 878 train loss:1.0002189494611313\n",
      "No. 879 train loss:0.8806111285254997\n",
      "No. 880 train loss:0.7759927502933127\n",
      "No. 881 train loss:1.0542464286260584\n",
      "No. 882 train loss:0.8653832208091246\n",
      "No. 883 train loss:0.7681034522393858\n",
      "No. 884 train loss:0.8883999484668326\n",
      "No. 885 train loss:0.8438063765011501\n",
      "No. 886 train loss:0.9675830896898451\n",
      "No. 887 train loss:1.0009105616432548\n",
      "No. 888 train loss:0.8579734426157143\n",
      "No. 889 train loss:1.0460625021581482\n",
      "No. 890 train loss:0.7762336438176919\n",
      "No. 891 train loss:0.9097787222459951\n",
      "No. 892 train loss:0.9389310381181707\n",
      "No. 893 train loss:1.0818094318078426\n",
      "No. 894 train loss:0.9765099806804696\n",
      "No. 895 train loss:0.9541796196262179\n",
      "No. 896 train loss:0.8289052857398183\n",
      "No. 897 train loss:0.8425245082581823\n",
      "No. 898 train loss:0.7331321223402063\n",
      "No. 899 train loss:0.8512056222995821\n",
      "No. 900 train loss:0.9075483168349255\n",
      "=== epoch:18, train acc:0.998, test acc:0.984 ===\n",
      "No. 901 train loss:0.8428452741018815\n",
      "No. 902 train loss:0.9270316793246383\n",
      "No. 903 train loss:1.1051766910564782\n",
      "No. 904 train loss:0.9206733320296165\n",
      "No. 905 train loss:0.7731566552699526\n",
      "No. 906 train loss:0.8430804623656009\n",
      "No. 907 train loss:0.8871172271450477\n",
      "No. 908 train loss:0.9015488368035238\n",
      "No. 909 train loss:0.8169713734217672\n",
      "No. 910 train loss:1.0238623253652381\n",
      "No. 911 train loss:0.966586677548472\n",
      "No. 912 train loss:0.8093613128438614\n",
      "No. 913 train loss:1.052180447965091\n",
      "No. 914 train loss:1.051402899505272\n",
      "No. 915 train loss:0.9294175052940058\n",
      "No. 916 train loss:0.9510998397500094\n",
      "No. 917 train loss:0.8537785943401306\n",
      "No. 918 train loss:0.8416467251537609\n",
      "No. 919 train loss:0.9298257668886918\n",
      "No. 920 train loss:0.807239433520405\n",
      "No. 921 train loss:0.9585237703418562\n",
      "No. 922 train loss:0.9444239521703512\n",
      "No. 923 train loss:0.8687955160455786\n",
      "No. 924 train loss:0.8875509209933503\n",
      "No. 925 train loss:0.9759167068472885\n",
      "No. 926 train loss:0.8682406354848343\n",
      "No. 927 train loss:0.7432417859683871\n",
      "No. 928 train loss:0.9559873879364389\n",
      "No. 929 train loss:0.7741347361311106\n",
      "No. 930 train loss:0.9196924203253376\n",
      "No. 931 train loss:0.6932026832512416\n",
      "No. 932 train loss:0.9782623378178341\n",
      "No. 933 train loss:0.8158067797427982\n",
      "No. 934 train loss:0.9986617118367366\n",
      "No. 935 train loss:1.1238647721017485\n",
      "No. 936 train loss:0.932935613401108\n",
      "No. 937 train loss:0.8988550623497954\n",
      "No. 938 train loss:0.8267009304932601\n",
      "No. 939 train loss:0.8608752336255194\n",
      "No. 940 train loss:0.9306478850144029\n",
      "No. 941 train loss:0.798151080169943\n",
      "No. 942 train loss:0.8401725737417849\n",
      "No. 943 train loss:0.9393514667735947\n",
      "No. 944 train loss:0.8577096449462954\n",
      "No. 945 train loss:1.016033610867734\n",
      "No. 946 train loss:0.9198780220487195\n",
      "No. 947 train loss:0.7607376493181006\n",
      "No. 948 train loss:0.9254808709736869\n",
      "No. 949 train loss:0.6600944270263698\n",
      "No. 950 train loss:0.7429454482831888\n",
      "=== epoch:19, train acc:0.999, test acc:0.983 ===\n",
      "No. 951 train loss:0.9183737821959933\n",
      "No. 952 train loss:0.8293246815247458\n",
      "No. 953 train loss:0.9590700061198753\n",
      "No. 954 train loss:0.8919829763145536\n",
      "No. 955 train loss:0.7007656609189108\n",
      "No. 956 train loss:0.9066198383546742\n",
      "No. 957 train loss:0.8280037192468428\n",
      "No. 958 train loss:0.9550926374156213\n",
      "No. 959 train loss:1.024653905868516\n",
      "No. 960 train loss:0.8220056998487405\n",
      "No. 961 train loss:0.9158339267066168\n",
      "No. 962 train loss:0.9273378994269907\n",
      "No. 963 train loss:0.8321833902991602\n",
      "No. 964 train loss:0.8427973960363877\n",
      "No. 965 train loss:0.9246466249015513\n",
      "No. 966 train loss:1.0102907912424788\n",
      "No. 967 train loss:0.9105485433700747\n",
      "No. 968 train loss:1.003595764042873\n",
      "No. 969 train loss:0.8746778056490621\n",
      "No. 970 train loss:0.9524217949872364\n",
      "No. 971 train loss:0.6857042492334933\n",
      "No. 972 train loss:0.9437753271233424\n",
      "No. 973 train loss:0.9176304067039187\n",
      "No. 974 train loss:0.9398805324780289\n",
      "No. 975 train loss:0.8589247562434629\n",
      "No. 976 train loss:1.0130763930160753\n",
      "No. 977 train loss:0.951618386632617\n",
      "No. 978 train loss:0.7361183972663022\n",
      "No. 979 train loss:0.945568465132272\n",
      "No. 980 train loss:0.8318453285967766\n",
      "No. 981 train loss:0.8580686445101625\n",
      "No. 982 train loss:0.8580778595770048\n",
      "No. 983 train loss:0.8427175770394593\n",
      "No. 984 train loss:0.8735493953422815\n",
      "No. 985 train loss:0.9487486664506214\n",
      "No. 986 train loss:0.9125851714550829\n",
      "No. 987 train loss:0.8093092847779905\n",
      "No. 988 train loss:0.8639141840646966\n",
      "No. 989 train loss:0.8576462057603174\n",
      "No. 990 train loss:0.8699293576626584\n",
      "No. 991 train loss:0.9910419182777231\n",
      "No. 992 train loss:0.895382906219738\n",
      "No. 993 train loss:0.927268891076475\n",
      "No. 994 train loss:0.8154455246227442\n",
      "No. 995 train loss:0.9471657131271611\n",
      "No. 996 train loss:0.8188637533857456\n",
      "No. 997 train loss:1.0331964575906214\n",
      "No. 998 train loss:0.9395297976351933\n",
      "No. 999 train loss:1.0105250132981949\n",
      "No. 1000 train loss:0.8006296014683656\n",
      "=== epoch:20, train acc:0.996, test acc:0.981 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.981\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/fElEQVR4nO3deXhTdd7//1eaNklLaYECXRBKUUQRRCmKgIwrRXR0cBlQZwTcfsOIg4ALIjMjcnsJ6uiIMqCO4HKPt3Ir4DAjt1pv2RREwOJCucGvVIvQUsvSvWmbnN8faQOhW5qmTXN4Pq7rXEk++ZyT98lJc179nJPEYhiGIQAAAJOICHUBAAAAwUS4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAAphLScLNx40Zde+21SklJkcVi0XvvvdfsPBs2bFB6erocDof69eunF198se0LBQAAYSOk4aasrExDhgzR4sWL/eqfk5Ojq6++WqNHj1ZWVpYeeeQRTZ8+XStXrmzjSgEAQLiwdJQfzrRYLFq9erXGjx/faJ/Zs2drzZo12r17t7dt6tSp+uqrr7Rly5Z2qBIAAHR0kaEuoCW2bNmijIwMn7axY8dq2bJlqq6uVlRUVL15nE6nnE6n97bb7daRI0eUkJAgi8XS5jUDAIDWMwxDJSUlSklJUURE0weewirc5OfnKzEx0actMTFRNTU1KiwsVHJycr15FixYoMcee6y9SgQAAG1o//79Ou2005rsE1bhRlK90Za6o2qNjcLMmTNHs2bN8t4uKipSnz59tH//fsXFxbVdoWEiMztfC//n/3So+PjoVmKcXQ+PO0tjBiaFsLKmudyGfvvsSlWXHm7wfoukyNgE/WPWjbJGBH+ErrLapYKSShUUOXWopFL5xZUqKHYqv7hCh4qcOlRcqcKyKjV20DdJh9XFUtLo8o8ZnZWvBL9qsVikOEekusbY1CXGpi4xUeoaE6UunWzqEh2lrtE2RduscrkN1bgNudxu1RiG3G5DNS7jhPbjU43bkMvw3O92u73317gNFRQ7lbNvb7P1Xzd6mM7r01WdHVbFOaLUuXZyREUEZdTU7TZUWOrUgWPlOnisUgeOVXgv845V6EBRpapr3D7zJOmw/m1/RA5LTaPLrTQidYftL6qKTVGkxSJrhEWRERGyRlhktVpkjZAiI2rbLbXtdbetx/tH1La53YaqatyqqnHLWeNSlcstZ93t6trrLpectberalxyujzX3Se9fvyt/5fOJ/x+/bQnf173ZY5ExdojZY+KkM0aIVukVfbICNkiI2SvnTzXa9ujImS3WmWLtHjuj7LKZvX0q3YZKqmsVklljYora2qvH79d6qxWSUWNSp019Z7r9hRltSgiwuJ5XVk8l97b1uOvP2/7ya/LCIsiTnxdRlhkPeG+I2VOffdd83+z0d17K8JiUXmVS+VVNSqvcqva5W50nqZ0j7Vp/YOXBfqUNKi4uFi9e/dW586dm+0bVuEmKSlJ+fn5Pm0FBQWKjIxUQkLDf8h2u112u71ee1xc3Ckfbj74Nk8PvPedDFkVYY/xthc6pQfe+05LYzvrqkH1R8PaSo3LrVJnjYoralRcWa3iiurayxNv16i4olqlBTlaaXlIjrjqRpdXaUTpxucdKnEke//oj++AIk56Izj+hnD8DeV4u0UWFZY6lVfkCTJHyqqaWZtIWWyRslktSoxzKDneoaT4aCXHO2QvPaBpu34vh6Xp2t8evloxPfrqSHmVjpZV6UhZlY6W111W62h5lY6Ve5ZR4pZKSqXc0ipJzdXWOikq1Ma4PzZb/+VfPKNXvuhe774oq0VxjijFRUcpzhFZexmluOhIn/bOtW2OKKt+LnHqp6MV+uloee1lhQ4crVBVU2+8VociI6Xk+Gj16hqt07pGq1eFoZ45Lnnib8Pi5NITV5+ucy+8pCVPS5uoqQ1CztpgtHvHJvXc2Hz9sy8+TT3OHN7qx3cbhqpdhpw1Lm8Qc9YcD2Le6w3cX3VC3c5qt2ylB7TS5cfrxvmMDlZ2lyolyV07tZUISTbJZvNcs0Z4Xn8nvRbjoqNUUlmtHV99o65NBISjRmfdd+NlGtqn6/HwYT35PaZ+WGlrrqO5qlk0QXY1/tw7FaXIe76UtWsfn/aqGrcqqlwqq6pReVWNypy11+suq1wqc9bU9nF5+8TYrG22j/Xnn6OwCjcjRozQv/71L5+2jz76SMOGDWvwfJt2cWy/VH5YLsPQrgPFOlJepW4xNp3TK05Wi0WKSZC69A5NbQ1w1/73XVXj1qNrdqmhf1bq2v703i4lx0fLkDz/7Z/0X753FKDutquR9trLymqXT0A5ObiUVbn8Xo9zLAfksDf+hyrJ8yZacUS55fF+L7clHFERSo6PVpI3vHguPWEmWknxDiV0stV783IdqJQ1u/nabxsSK2uvpl87NS63iiqqa0NPtW8AKqvSkdrrldUuRVkjGn2DPXnUocHQV3u/66csOb5rvv5hPd3aFxXns43dhlTtMnS4rEqHmw2IzbNGWJQU59BpXaN1WteY2svj15PiHYqyHj827zpgSH9vfrnnxByVSvIlR7wU6fAMj4VApDVCkdYIdar9/6zngB7SxubnG39+L1l79Wjb4lro6y82yLG2+dfNvCsTlTjgokZDk7Pa5RP4vKNfJ/apcXtf83HRUersiGwwQJ8crB1R1kZrcx3NVc2e+5sPCGd8KWvX5kcW2pO14oisTdQtybNeFUekk8KNrXakLD4mRPvYAIU03JSWlur//b//572dk5OjnTt3qlu3burTp4/mzJmjAwcO6I033pDk+WTU4sWLNWvWLN19993asmWLli1bprfeeis0K3Bsv7Q4Xapxyirp3Ib6RNqle3f4FXDcbkNHy6v0c6lTP5ecNJU6dbi0Ss4aVwNhwt1A6DjxEMPx+1vy2bifS5361d8+83+GIImxWRt9I6p7k4o8VCVlN7+saZeersSzLqr/PLk8h118wlgT4c3tNtStk90nxMRHRwV0eMXq5zzW6jLJ7ZaaOHEu0hqhhFi7EmLrj062FdeBYum75vv9deJ5svY633vbMAyVVXkCruewwEmjcycH3trrZVU16tnZfkJ4OR5ikuIcirQ28Py4XVLRfumHHOnIvtopR9ZDu/xaR+u7k4/fiIjyhJxmpy61l3G+7RFRUk2lVONs5LKp+3wvrcUH/at/8/NS/GmeYBZpP+myobZGLq02yXBJ7prayVU71ZzU1tRtT9s51Xv9qv3KiC8VUdjA4WaLpKjayV+R9hO2i11ydPZcj3K0YCEerQkIATGMkIVqMwhpuNm+fbsuu+z4Mbm6c2MmT56s1157TXl5ecrNzfXen5aWprVr12rmzJn629/+ppSUFD3//PO68cYb2712SVL5Yc+bTlNqnCo7dkiHqrt6Q0ph7eWJweXnEqcKS6vkascDvykqbHaItTw6WbGOyPr/7VsbH2Zt7NCPLTKigdDie7uzI9LnP+16nKVS6SG5Ykv8Cjdj436Q1dJN6nTCziYqum3eNNwuz2ui9FDt9HPtZcEJbQVS8QH/lvfaNZIsJ+0su/juOO1xTe9w7XH1w5Hb3bKdquukHezRH/0q3/rTF5K72luLxRGvWJtDsfYgve3UVElHa4PL0RNDzD7p6I+exw6UvbNUVSYZbs9yygs9U7jYtSrUFdTT+JiIr4iNT7ZpHZIkq72Bv5eG/pa6HL9ekuffso/kSK5qqarU8xqqKmviehO3XU5JFiki8oTJGtjtqnL/as/6h/TjZwEGYXuT/4i1tw7zPTftpbi4WPHx8SoqKmr18UDXgSxZ/35ps/3+WH27st2pKpNDZXKowrCrTA5VyqaGjp1362RTj1i7enQ+YYq1KyHWphibVRGWps4b8SN8WC3alb1L5//zymaPf++68ROln9vgmFTw1FRJZQUnBYGTAkFdWKgua/3jNfif+MlvbF18b9tipcpjTddY9rNnZ9ihWDwBxxp1PLC0ZqffWlabf6HsxOc/yiEV/eTZaZwYYIr2N/18W21S1zSpWz+pW+2lYUj/82Dzdf5/G6TkIZ4dTWVRA1Nx7eWxRu6vnYyTDrVa/dxRNHZZfkTa9nLz9Z9/m+e583NEyPeyovnXcSA72BqnVODHfyS9L/L8PbaWYXjW6eRt0uDBeASF1Xb89do5SZr6aVAX35L9d1idc9PR7DpQ3PChqJM8HvVqg+2GLKqJjJE7spNk66QIR6ysjlhF2GIlWyfPDtXmuU/qJFXFei5tnaSIuvtifPtFdZKszW/WCxMNWZsINpLn+Pd5CX6eB9Pkfyq1tyuO+QaCstqRjYqj/j1GnagYz86vxI8h+u5nemo7cWfTpv+JW6RO3aXYRCm25/HLTidcrzgqvTO5+UXdkSl1TZWcfu5IT55qKiUZkrOoiXIjpMjo2p2n3b+drLNUyn6v+fq79z/puXdLrirPdi/72c/nsxlRMcfDizfI1E5xKZ4d64kO7vR/2RaLZwTH3tlziKelDMPz2ndXe55jq631/9ke3OlfuLngLinlvMAfx1Xjef24quqHFUtEYCOfB3dKL/txkva4J1tXe1Pc7iYC6wmTs4G28sOSs/GRbi9HFym6i+d9OSrm+Hvzie/T/lyPdNSOHPp5CNB7+LCBPkf2SesXNF/7gHGeGloSiE8Mwq4qz+SU5/UeQoSbVjhS7t8JkSUxfdTZHnF8R1/tGSK0yFBUTZlUU+b5ZEBxkAqLdDT7h2Ot8m8ExPr5Ek+AanYItZUnh0ZE1g8EsYknhYPaNnus/2+UN/z9+Btl3c7G+wZWfNIb2LEm3uxKPP8JN1VjbKIU0735cOnvDjbSJnVO9EyBqK6sXcdizxC3d5j5xKHkAN4CDu70L9zc8MpJz70fO5WGpqoyT1DxBpcTQkxsYsc9L8Fi8bxWw5E1UrKGae1NiYioHaWNk9TCD3r4+54z6Z9tF84CdXCnf+HmkodbVrtheAJUg6EntCNkhJtW6BbjXzLNuXSx78dK3S5PwPEJCc0dmy2tncobDxl1Q+B151OUN/wdMC3yzX+3rH9EZG2IOjFU1QYrR1zDgSA20fPfTlsfr63b2dhjpfhebftYHUGUwzPF9gx1Ja0fBQmWmARPsGvqXLlIu6dfRxTu9cNcLBbPIW9rlOdvuwMh3LTCOb38Oy5cr1+E9fgbfbAYhucNr6FQVN1AIDqyT/p6RfPLHXKL5z/kE0OKd6i1gQAT2U5DkeH8Jh/OtUvhXX+X3p5PLzYV/DvY1zf4COf6w/l1E+5OweeecNMKfn+ktz2GzS2W4/+pd/LjBXpwp3/hZvjUjjfEKoX3m3w41y6Zo/6OWps/wrX+cH/dhHNACPfnPgCEm9YI5xe7GYTrm7wU3rVL4V8/QiOcXzfhHhDC+bkPAOGmNU54sYfLNxQDAAJ0igWEcEa4aa3aF7tV0rnhdI4qo04AAJMi3Jyqwn2IFQCARhBuTmUMsQIATKjj/BAEAABAEBBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqRBuAACAqYQ83CxZskRpaWlyOBxKT0/Xpk2bmuz/5ptvasiQIYqJiVFycrJuv/12HT58uJ2qBQAAHV1Iw82KFSs0Y8YMzZ07V1lZWRo9erTGjRun3NzcBvt/+umnmjRpku68807t2rVL77zzjrZt26a77rqrnSsHAAAdVUjDzbPPPqs777xTd911l84++2w999xz6t27t5YuXdpg/88//1x9+/bV9OnTlZaWposvvli/+93vtH379nauHAAAdFQhCzdVVVXasWOHMjIyfNozMjK0efPmBucZOXKkfvrpJ61du1aGYejQoUN69913dc011zT6OE6nU8XFxT4TAAAwr5CFm8LCQrlcLiUmJvq0JyYmKj8/v8F5Ro4cqTfffFMTJ06UzWZTUlKSunTpohdeeKHRx1mwYIHi4+O9U+/evYO6HgAAoGMJ+QnFFovF57ZhGPXa6mRnZ2v69On685//rB07duiDDz5QTk6Opk6d2ujy58yZo6KiIu+0f//+oNYPAAA6lshQPXD37t1ltVrrjdIUFBTUG82ps2DBAo0aNUoPPvigJOncc89Vp06dNHr0aD3++ONKTk6uN4/dbpfdbg/+CgAAgA4pZCM3NptN6enpyszM9GnPzMzUyJEjG5ynvLxcERG+JVutVkmeER8AAICQHpaaNWuWXnnlFS1fvly7d+/WzJkzlZub6z3MNGfOHE2aNMnb/9prr9WqVau0dOlS7du3T5999pmmT5+uCy+8UCkpKaFaDQAA0IGE7LCUJE2cOFGHDx/W/PnzlZeXp0GDBmnt2rVKTU2VJOXl5fl8582UKVNUUlKixYsX6/7771eXLl10+eWX68knnwzVKgAAgA7GYpxix3OKi4sVHx+voqIixcXFhbocAADgh5bsv0P+aSkAAIBgItwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTIdwAAABTCXm4WbJkidLS0uRwOJSenq5NmzY12d/pdGru3LlKTU2V3W7X6aefruXLl7dTtQAAoKOLDOWDr1ixQjNmzNCSJUs0atQovfTSSxo3bpyys7PVp0+fBueZMGGCDh06pGXLlumMM85QQUGBampq2rlyAADQUVkMwzBC9eDDhw/X0KFDtXTpUm/b2WefrfHjx2vBggX1+n/wwQe6+eabtW/fPnXr1i2gxywuLlZ8fLyKiooUFxcXcO0AAKD9tGT/HbLDUlVVVdqxY4cyMjJ82jMyMrR58+YG51mzZo2GDRump556Sr169dKZZ56pBx54QBUVFY0+jtPpVHFxsc8EAADMK2SHpQoLC+VyuZSYmOjTnpiYqPz8/Abn2bdvnz799FM5HA6tXr1ahYWFuueee3TkyJFGz7tZsGCBHnvssaDXDwAAOqaQn1BssVh8bhuGUa+tjtvtlsVi0ZtvvqkLL7xQV199tZ599lm99tprjY7ezJkzR0VFRd5p//79QV8HAADQcYRs5KZ79+6yWq31RmkKCgrqjebUSU5OVq9evRQfH+9tO/vss2UYhn766Sf179+/3jx2u112uz24xQMAgA4rZCM3NptN6enpyszM9GnPzMzUyJEjG5xn1KhROnjwoEpLS71te/fuVUREhE477bQ2rRcAAISHkB6WmjVrll555RUtX75cu3fv1syZM5Wbm6upU6dK8hxSmjRpkrf/rbfeqoSEBN1+++3Kzs7Wxo0b9eCDD+qOO+5QdHR0qFYDAAB0ICH9npuJEyfq8OHDmj9/vvLy8jRo0CCtXbtWqampkqS8vDzl5uZ6+8fGxiozM1N/+MMfNGzYMCUkJGjChAl6/PHHQ7UKAACggwnp99yEAt9zAwBA+AmL77kBAABoCwGFm/Xr1we5DAAAgOAIKNxcddVVOv300/X444/zvTEAAKBDCSjcHDx4UPfdd59WrVqltLQ0jR07Vv/93/+tqqqqYNcHAADQIgGFm27dumn69On68ssvtX37dg0YMEDTpk1TcnKypk+frq+++irYdQIAAPil1ScUn3feeXr44Yc1bdo0lZWVafny5UpPT9fo0aO1a9euYNQIAADgt4DDTXV1td59911dffXVSk1N1YcffqjFixfr0KFDysnJUe/evfXrX/86mLUCAAA0K6Av8fvDH/6gt956S5L029/+Vk899ZQGDRrkvb9Tp05auHCh+vbtG5QiAQAA/BVQuMnOztYLL7ygG2+8UTabrcE+KSkpWrduXauKAwAAaCm+oRgAAHR4bf4NxQsWLNDy5cvrtS9fvlxPPvlkIIsEAAAIioDCzUsvvaSzzjqrXvs555yjF198sdVFAQAABCqgcJOfn6/k5OR67T169FBeXl6riwIAAAhUQOGmd+/e+uyzz+q1f/bZZ0pJSWl1UQAAAIEK6NNSd911l2bMmKHq6mpdfvnlkqT//d//1UMPPaT7778/qAUCAAC0REDh5qGHHtKRI0d0zz33eH9PyuFwaPbs2ZozZ05QCwQAAGiJVn0UvLS0VLt371Z0dLT69+8vu90ezNraBB8FBwAg/LRk/x3QyE2d2NhYXXDBBa1ZBAAAQFAFHG62bdumd955R7m5ud5DU3VWrVrV6sIAAAACEdCnpd5++22NGjVK2dnZWr16taqrq5Wdna1PPvlE8fHxwa4RAADAbwGFmyeeeEJ//etf9e9//1s2m02LFi3S7t27NWHCBPXp0yfYNQIAAPgtoHDz/fff65prrpEk2e12lZWVyWKxaObMmXr55ZeDWiAAAEBLBBRuunXrppKSEklSr1699O2330qSjh07pvLy8uBVBwAA0EIBnVA8evRoZWZmavDgwZowYYLuu+8+ffLJJ8rMzNQVV1wR7BoBAAD8FlC4Wbx4sSorKyVJc+bMUVRUlD799FPdcMMN+tOf/hTUAgEAAFqixV/iV1NTozfffFNjx45VUlJSW9XVZvgSPwAAwk9L9t8tPucmMjJSv//97+V0OgMuEAAAoK0EdELx8OHDlZWVFexaAAAAWi2gc27uuece3X///frpp5+Unp6uTp06+dx/7rnnBqU4AACAlgrohzMjIuoP+FgsFhmGIYvFIpfLFZTi2gLn3AAAEH7a/Iczc3JyAioMAACgrQUUblJTU4NdBwAAQFAEFG7eeOONJu+fNGlSQMUAAAC0VkDn3HTt2tXndnV1tcrLy2Wz2RQTE6MjR44ErcBg45wbAADCT5t+z40kHT161GcqLS3Vnj17dPHFF+utt94KqGgAAIBgCCjcNKR///5auHCh7rvvvmAtEgAAoMWCFm4kyWq16uDBg8FcJAAAQIsEdELxmjVrfG4bhqG8vDwtXrxYo0aNCkphAAAAgQgo3IwfP97ntsViUY8ePXT55ZfrmWeeCUZdAAAAAQko3Ljd7mDXAQAAEBRBPecGAAAg1AIKNzfddJMWLlxYr/3pp5/Wr3/961YXBQAAEKiAws2GDRt0zTXX1Gu/6qqrtHHjxlYXBQAAEKiAwk1paalsNlu99qioKBUXF7e6KAAAgEAFFG4GDRqkFStW1Gt/++23NXDgwFYXBQAAEKiAPi31pz/9STfeeKO+//57XX755ZKk//3f/9Vbb72ld955J6gFAgAAtERA4ea6667Te++9pyeeeELvvvuuoqOjde655+rjjz/WJZdcEuwaAQAA/BbQr4KHM34VHACA8NPmvwq+bds2bd26tV771q1btX379kAWCQAAEBQBhZtp06Zp//799doPHDigadOmtbooAACAQAUUbrKzszV06NB67eeff76ys7NbXRQAAECgAgo3drtdhw4dqteel5enyMiAzlEGAAAIioDCzZgxYzRnzhwVFRV5244dO6ZHHnlEY8aMCVpxAAAALRXQMMszzzyjX/ziF0pNTdX5558vSdq5c6cSExP1n//5n0EtEAAAoCUCCje9evXS119/rTfffFNfffWVoqOjdfvtt+uWW25RVFRUsGsEAADwW8AnyHTq1EkXX3yx+vTpo6qqKknS//zP/0jyfMkfAABAKAQUbvbt26frr79e33zzjSwWiwzDkMVi8d7vcrmCViAAAEBLBHRC8X333ae0tDQdOnRIMTEx+vbbb7VhwwYNGzZM69evD3KJAAAA/gto5GbLli365JNP1KNHD0VERMhqteriiy/WggULNH36dGVlZQW7TgAAAL8ENHLjcrkUGxsrSerevbsOHjwoSUpNTdWePXuCVx0AAEALBTRyM2jQIH399dfq16+fhg8frqeeeko2m00vv/yy+vXrF+waAQAA/BZQuPnjH/+osrIySdLjjz+uX/7ylxo9erQSEhK0YsWKoBYIAADQEhbDMIxgLOjIkSPq2rWrz6emOqKW/GQ6AADoGFqy/w7onJuGdOvWLaBgs2TJEqWlpcnhcCg9PV2bNm3ya77PPvtMkZGROu+881r8mAAAwLyCFm4CsWLFCs2YMUNz585VVlaWRo8erXHjxik3N7fJ+YqKijRp0iRdccUV7VQpAAAIF0E7LBWI4cOHa+jQoVq6dKm37eyzz9b48eO1YMGCRue7+eab1b9/f1mtVr333nvauXOn34/JYSkAAMJPSA5LtVRVVZV27NihjIwMn/aMjAxt3ry50fleffVVff/993r00Uf9ehyn06ni4mKfCQAAmFfIwk1hYaFcLpcSExN92hMTE5Wfn9/gPN99950efvhhvfnmm4qM9O+DXgsWLFB8fLx36t27d6trBwAAHVdIz7mRVO8k5JN/p6qOy+XSrbfeqscee0xnnnmm38ufM2eOioqKvNP+/ftbXTMAAOi4Av5V8Nbq3r27rFZrvVGagoKCeqM5klRSUqLt27crKytL9957ryTJ7XbLMAxFRkbqo48+0uWXX15vPrvdLrvd3jYrAQAAOpyQjdzYbDalp6crMzPTpz0zM1MjR46s1z8uLk7ffPONdu7c6Z2mTp2qAQMGaOfOnRo+fHh7lQ4AADqwkI3cSNKsWbN02223adiwYRoxYoRefvll5ebmaurUqZI8h5QOHDigN954QxERERo0aJDP/D179pTD4ajXDgAATl0hDTcTJ07U4cOHNX/+fOXl5WnQoEFau3atUlNTJUl5eXnNfucNAADAiUL6PTehwPfcAAAQfsLie24AAADaAuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYSsjDzZIlS5SWliaHw6H09HRt2rSp0b6rVq3SmDFj1KNHD8XFxWnEiBH68MMP27FaAADQ0YU03KxYsUIzZszQ3LlzlZWVpdGjR2vcuHHKzc1tsP/GjRs1ZswYrV27Vjt27NBll12ma6+9VllZWe1cOQAA6KgshmEYoXrw4cOHa+jQoVq6dKm37eyzz9b48eO1YMECv5ZxzjnnaOLEifrzn//sV//i4mLFx8erqKhIcXFxAdUNAADaV0v23yEbuamqqtKOHTuUkZHh056RkaHNmzf7tQy3262SkhJ169at0T5Op1PFxcU+EwAAMK+QhZvCwkK5XC4lJib6tCcmJio/P9+vZTzzzDMqKyvThAkTGu2zYMECxcfHe6fevXu3qm4AANCxhfyEYovF4nPbMIx6bQ156623NG/ePK1YsUI9e/ZstN+cOXNUVFTknfbv39/qmgEAQMcVGaoH7t69u6xWa71RmoKCgnqjOSdbsWKF7rzzTr3zzju68sorm+xrt9tlt9tbXS8AAAgPIRu5sdlsSk9PV2Zmpk97ZmamRo4c2eh8b731lqZMmaL/+q//0jXXXNPWZQIAgDATspEbSZo1a5Zuu+02DRs2TCNGjNDLL7+s3NxcTZ06VZLnkNKBAwf0xhtvSPIEm0mTJmnRokW66KKLvKM+0dHRio+PD9l6AACAjiOk4WbixIk6fPiw5s+fr7y8PA0aNEhr165VamqqJCkvL8/nO29eeukl1dTUaNq0aZo2bZq3ffLkyXrttdfau3wAANABhfR7bkKB77kBACD8hMX33AAAALQFwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADCVyFAXAACAmbhcLlVXV4e6jLBks9kUEdH6cRfCDQAAQWAYhvLz83Xs2LFQlxK2IiIilJaWJpvN1qrlEG4AAAiCumDTs2dPxcTEyGKxhLqksOJ2u3Xw4EHl5eWpT58+rXr+CDcAALSSy+XyBpuEhIRQlxO2evTooYMHD6qmpkZRUVEBL4cTigEAaKW6c2xiYmJCXEl4qzsc5XK5WrUcwg0AAEHCoajWCdbzR7gBAACmQrgBAKCDcLkNbfn+sP6584C2fH9YLrcR6pJapG/fvnruuedCXQYnFAMA0BF88G2eHvtXtvKKKr1tyfEOPXrtQF01KLnNHvfSSy/VeeedF5RQsm3bNnXq1Kn1RbUSIzcAAITYB9/m6ff/+NIn2EhSflGlfv+PL/XBt3khqszz/T01NTV+9e3Ro0eHOKmacAMAQBswDEPlVTXNTiWV1Xp0zS41dACqrm3emmyVVFb7tTzD8P9Q1pQpU7RhwwYtWrRIFotFFotFr732miwWiz788EMNGzZMdrtdmzZt0vfff69f/epXSkxMVGxsrC644AJ9/PHHPss7+bCUxWLRK6+8ouuvv14xMTHq37+/1qxZ0/Ins4U4LAUAQBuoqHZp4J8/bPVyDEn5xZUaPO8jv/pnzx+rGJt/u/dFixZp7969GjRokObPny9J2rVrlyTpoYce0l/+8hf169dPXbp00U8//aSrr75ajz/+uBwOh15//XVde+212rNnj/r06dPoYzz22GN66qmn9PTTT+uFF17Qb37zG/3444/q1q2bXzUGgpEbAABOUfHx8bLZbIqJiVFSUpKSkpJktVolSfPnz9eYMWN0+umnKyEhQUOGDNHvfvc7DR48WP3799fjjz+ufv36NTsSM2XKFN1yyy0644wz9MQTT6isrExffPFFm64XIzcAALSB6CirsuePbbbfFzlHNOXVbc32e+32C3RhWvOjHdFRVr/qa86wYcN8bpeVlemxxx7Tv//9b++3CFdUVCg3N7fJ5Zx77rne6506dVLnzp1VUFAQlBobQ7gBAKANWCwWvw4Pje7fQ8nxDuUXVTZ43o1FUlK8Q6P795A1ov2+JPDkTz09+OCD+vDDD/WXv/xFZ5xxhqKjo3XTTTepqqqqyeWc/DMKFotFbrc76PWeiMNSAACEkDXCokevHSjJE2ROVHf70WsHtlmwsdlsfv3cwaZNmzRlyhRdf/31Gjx4sJKSkvTDDz+0SU2tRbgBACDErhqUrKW/HaqkeIdPe1K8Q0t/O7RNv+emb9++2rp1q3744QcVFhY2OqpyxhlnaNWqVdq5c6e++uor3XrrrW0+AhMoDksBANABXDUoWWMGJumLnCMqKKlUz84OXZjWrc0PRT3wwAOaPHmyBg4cqIqKCr366qsN9vvrX/+qO+64QyNHjlT37t01e/ZsFRcXt2ltgbIYLflAvAkUFxcrPj5eRUVFiouLC3U5AAATqKysVE5OjtLS0uRwOJqfAQ1q6nlsyf6bw1IAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBU+PkFAABC7dh+qfxw4/fHJEhderdfPWGOcAMAQCgd2y8tTpdqnI33ibRL9+5ok4Bz6aWX6rzzztNzzz0XlOVNmTJFx44d03vvvReU5QWCw1IAAIRS+eGmg43kub+pkR34INwAANAWDEOqKmt+qqnwb3k1Ff4trwW/hz1lyhRt2LBBixYtksVikcVi0Q8//KDs7GxdffXVio2NVWJiom677TYVFhZ653v33Xc1ePBgRUdHKyEhQVdeeaXKyso0b948vf766/rnP//pXd769etb+MS1HoelAABoC9Xl0hMpwVve8qv86/fIQcnWya+uixYt0t69ezVo0CDNnz9fkuRyuXTJJZfo7rvv1rPPPquKigrNnj1bEyZM0CeffKK8vDzdcssteuqpp3T99derpKREmzZtkmEYeuCBB7R7924VFxfr1VdflSR169YtoNVtDcINAACnqPj4eNlsNsXExCgpKUmS9Oc//1lDhw7VE0884e23fPly9e7dW3v37lVpaalqamp0ww03KDU1VZI0ePBgb9/o6Gg5nU7v8kKBcAMAQFuIivGMojQn/2v/RmXu+EBKOte/x22FHTt2aN26dYqNja133/fff6+MjAxdccUVGjx4sMaOHauMjAzddNNN6tq1a6seN5gINwAAtAWLxb/DQ5HR/i0vMtrvw02t4Xa7de211+rJJ5+sd19ycrKsVqsyMzO1efNmffTRR3rhhRc0d+5cbd26VWlpaW1enz84oRgAgFOYzWaTy+Xy3h46dKh27dqlvn376owzzvCZOnXyhCuLxaJRo0bpscceU1ZWlmw2m1avXt3g8kKBcAMAQCjFJHi+x6YpkXZPvzbQt29fbd26VT/88IMKCws1bdo0HTlyRLfccou++OIL7du3Tx999JHuuOMOuVwubd26VU888YS2b9+u3NxcrVq1Sj///LPOPvts7/K+/vpr7dmzR4WFhaqurm6TupvCYSkAAEKpS2/PF/SF6BuKH3jgAU2ePFkDBw5URUWFcnJy9Nlnn2n27NkaO3asnE6nUlNTddVVVykiIkJxcXHauHGjnnvuORUXFys1NVXPPPOMxo0bJ0m6++67tX79eg0bNkylpaVat26dLr300japvTEWw2jBB+JNoLi4WPHx8SoqKlJcXFyoywEAmEBlZaVycnKUlpYmh8MR6nLCVlPPY0v23xyWAgAApkK4AQAApkK4AQAApkK4AQAApkK4AQAgSE6xz+gEXbCeP8INAACtFBUVJUkqLy8PcSXhraqqSpJktVpbtRy+5wYAgFayWq3q0qWLCgoKJEkxMTGyWCwhriq8uN1u/fzzz4qJiVFkZOviCeEGAIAgqPsV7LqAg5aLiIhQnz59Wh0MCTcAAASBxWJRcnKyevbsGZKfHDADm82miIjWnzFDuAEAIIisVmurzxlB64T8hOIlS5Z4v2Y5PT1dmzZtarL/hg0blJ6eLofDoX79+unFF19sp0oBAEA4CGm4WbFihWbMmKG5c+cqKytLo0eP1rhx45Sbm9tg/5ycHF199dUaPXq0srKy9Mgjj2j69OlauXJlO1cOAAA6qpD+cObw4cM1dOhQLV261Nt29tlna/z48VqwYEG9/rNnz9aaNWu0e/dub9vUqVP11VdfacuWLX49Jj+cCQBA+GnJ/jtk59xUVVVpx44devjhh33aMzIytHnz5gbn2bJlizIyMnzaxo4dq2XLlqm6utr7PQMncjqdcjqd3ttFRUWSPE8SAAAID3X7bX/GZEIWbgoLC+VyuZSYmOjTnpiYqPz8/Abnyc/Pb7B/TU2NCgsLlZycXG+eBQsW6LHHHqvX3rt371ZUDwAAQqGkpETx8fFN9gn5p6VO/iy7YRhNfr69of4NtdeZM2eOZs2a5b3tdrt15MgRJSQkBP0LloqLi9W7d2/t37/f9Ie8TqV1lU6t9WVdzetUWl/W1XwMw1BJSYlSUlKa7RuycNO9e3dZrdZ6ozQFBQX1RmfqJCUlNdg/MjJSCQkJDc5jt9tlt9t92rp06RJ44X6Ii4sz9QvsRKfSukqn1vqyruZ1Kq0v62ouzY3Y1AnZp6VsNpvS09OVmZnp056ZmamRI0c2OM+IESPq9f/oo480bNiwBs+3AQAAp56QfhR81qxZeuWVV7R8+XLt3r1bM2fOVG5urqZOnSrJc0hp0qRJ3v5Tp07Vjz/+qFmzZmn37t1avny5li1bpgceeCBUqwAAADqYkJ5zM3HiRB0+fFjz589XXl6eBg0apLVr1yo1NVWSlJeX5/OdN2lpaVq7dq1mzpypv/3tb0pJSdHzzz+vG2+8MVSr4MNut+vRRx+tdxjMjE6ldZVOrfVlXc3rVFpf1vXUFtLvuQEAAAi2kP/8AgAAQDARbgAAgKkQbgAAgKkQbgAAgKkQblpoyZIlSktLk8PhUHp6ujZt2tRk/w0bNig9PV0Oh0P9+vXTiy++2E6VBm7BggW64IIL1LlzZ/Xs2VPjx4/Xnj17mpxn/fr1slgs9ab/+7//a6eqAzdv3rx6dSclJTU5TzhuV0nq27dvg9tp2rRpDfYPp+26ceNGXXvttUpJSZHFYtF7773nc79hGJo3b55SUlIUHR2tSy+9VLt27Wp2uStXrtTAgQNlt9s1cOBArV69uo3WoGWaWt/q6mrNnj1bgwcPVqdOnZSSkqJJkybp4MGDTS7ztddea3B7V1ZWtvHaNK25bTtlypR6NV900UXNLrcjbtvm1rWh7WOxWPT00083usyOul3bEuGmBVasWKEZM2Zo7ty5ysrK0ujRozVu3Difj6ufKCcnR1dffbVGjx6trKwsPfLII5o+fbpWrlzZzpW3zIYNGzRt2jR9/vnnyszMVE1NjTIyMlRWVtbsvHv27FFeXp536t+/fztU3HrnnHOOT93ffPNNo33DdbtK0rZt23zWs+5LMX/96183OV84bNeysjINGTJEixcvbvD+p556Ss8++6wWL16sbdu2KSkpSWPGjFFJSUmjy9yyZYsmTpyo2267TV999ZVuu+02TZgwQVu3bm2r1fBbU+tbXl6uL7/8Un/605/05ZdfatWqVdq7d6+uu+66ZpcbFxfns63z8vLkcDjaYhX81ty2laSrrrrKp+a1a9c2ucyOum2bW9eTt83y5ctlsVia/UqUjrhd25QBv1144YXG1KlTfdrOOuss4+GHH26w/0MPPWScddZZPm2/+93vjIsuuqjNamwLBQUFhiRjw4YNjfZZt26dIck4evRo+xUWJI8++qgxZMgQv/ubZbsahmHcd999xumnn2643e4G7w/X7SrJWL16tfe22+02kpKSjIULF3rbKisrjfj4eOPFF19sdDkTJkwwrrrqKp+2sWPHGjfffHPQa26Nk9e3IV988YUhyfjxxx8b7fPqq68a8fHxwS0uyBpa18mTJxu/+tWvWrSccNi2/mzXX/3qV8bll1/eZJ9w2K7BxsiNn6qqqrRjxw5lZGT4tGdkZGjz5s0NzrNly5Z6/ceOHavt27erurq6zWoNtqKiIklSt27dmu17/vnnKzk5WVdccYXWrVvX1qUFzXfffaeUlBSlpaXp5ptv1r59+xrta5btWlVVpX/84x+64447mv0R2XDdrnVycnKUn5/vs93sdrsuueSSRv9+pca3dVPzdFRFRUWyWCzN/rZeaWmpUlNTddppp+mXv/ylsrKy2qfAVlq/fr169uypM888U3fffbcKCgqa7G+GbXvo0CG9//77uvPOO5vtG67bNVCEGz8VFhbK5XLV+1HPxMTEej/mWSc/P7/B/jU1NSosLGyzWoPJMAzNmjVLF198sQYNGtRov+TkZL388stauXKlVq1apQEDBuiKK67Qxo0b27HawAwfPlxvvPGGPvzwQ/39739Xfn6+Ro4cqcOHDzfY3wzbVZLee+89HTt2TFOmTGm0Tzhv1xPV/Y225O+3br6WztMRVVZW6uGHH9att97a5A8rnnXWWXrttde0Zs0avfXWW3I4HBo1apS+++67dqy25caNG6c333xTn3zyiZ555hlt27ZNl19+uZxOZ6PzmGHbvv766+rcubNuuOGGJvuF63ZtjZD+/EI4Ovk/XMMwmvyvt6H+DbV3VPfee6++/vprffrpp032GzBggAYMGOC9PWLECO3fv19/+ctf9Itf/KKty2yVcePGea8PHjxYI0aM0Omnn67XX39ds2bNanCecN+ukrRs2TKNGzdOKSkpjfYJ5+3akJb+/QY6T0dSXV2tm2++WW63W0uWLGmy70UXXeRzIu6oUaM0dOhQvfDCC3r++efbutSATZw40Xt90KBBGjZsmFJTU/X+++83ueMP9227fPly/eY3v2n23Jlw3a6twciNn7p37y6r1Vov1RcUFNRL/3WSkpIa7B8ZGamEhIQ2qzVY/vCHP2jNmjVat26dTjvttBbPf9FFF4XlfwadOnXS4MGDG6093LerJP3444/6+OOPddddd7V43nDcrnWffmvJ32/dfC2dpyOprq7WhAkTlJOTo8zMzCZHbRoSERGhCy64IOy2d3JyslJTU5usO9y37aZNm7Rnz56A/obDdbu2BOHGTzabTenp6d5Pl9TJzMzUyJEjG5xnxIgR9fp/9NFHGjZsmKKiotqs1tYyDEP33nuvVq1apU8++URpaWkBLScrK0vJyclBrq7tOZ1O7d69u9Haw3W7nujVV19Vz549dc0117R43nDcrmlpaUpKSvLZblVVVdqwYUOjf79S49u6qXk6irpg89133+njjz8OKHgbhqGdO3eG3fY+fPiw9u/f32Td4bxtJc/Ia3p6uoYMGdLiecN1u7ZIqM5kDkdvv/22ERUVZSxbtszIzs42ZsyYYXTq1Mn44YcfDMMwjIcffti47bbbvP337dtnxMTEGDNnzjSys7ONZcuWGVFRUca7774bqlXwy+9//3sjPj7eWL9+vZGXl+edysvLvX1OXte//vWvxurVq429e/ca3377rfHwww8bkoyVK1eGYhVa5P777zfWr19v7Nu3z/j888+NX/7yl0bnzp1Nt13ruFwuo0+fPsbs2bPr3RfO27WkpMTIysoysrKyDEnGs88+a2RlZXk/HbRw4UIjPj7eWLVqlfHNN98Yt9xyi5GcnGwUFxd7l3Hbbbf5fPrxs88+M6xWq7Fw4UJj9+7dxsKFC43IyEjj888/b/f1O1lT61tdXW1cd911xmmnnWbs3LnT5+/Y6XR6l3Hy+s6bN8/44IMPjO+//97Iysoybr/9diMyMtLYunVrKFbRq6l1LSkpMe6//35j8+bNRk5OjrFu3TpjxIgRRq9evcJy2zb3OjYMwygqKjJiYmKMpUuXNriMcNmubYlw00J/+9vfjNTUVMNmsxlDhw71+Xj05MmTjUsuucSn//r1643zzz/fsNlsRt++fRt9MXYkkhqcXn31VW+fk9f1ySefNE4//XTD4XAYXbt2NS6++GLj/fffb//iAzBx4kQjOTnZiIqKMlJSUowbbrjB2LVrl/d+s2zXOh9++KEhydizZ0+9+8J5u9Z9bP3kafLkyYZheD4O/uijjxpJSUmG3W43fvGLXxjffPONzzIuueQSb/8677zzjjFgwAAjKirKOOusszpMsGtqfXNychr9O163bp13GSev74wZM4w+ffoYNpvN6NGjh5GRkWFs3ry5/VfuJE2ta3l5uZGRkWH06NHDiIqKMvr06WNMnjzZyM3N9VlGuGzb5l7HhmEYL730khEdHW0cO3aswWWEy3ZtSxbDqD0TEgAAwAQ45wYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QbAKWf9+vWyWCw6duxYqEsB0AYINwAAwFQINwAAwFQINwDanWEYeuqpp9SvXz9FR0dryJAhevfddyUdP2T0/vvva8iQIXI4HBo+fLi++eYbn2WsXLlS55xzjux2u/r27atnnnnG536n06mHHnpIvXv3lt1uV//+/bVs2TKfPjt27NCwYcMUExOjkSNHas+ePd77vvrqK1122WXq3Lmz4uLilJ6eru3bt7fRMwIgmCJDXQCAU88f//hHrVq1SkuXLlX//v21ceNG/fa3v1WPHj28fR588EEtWrRISUlJeuSRR3Tddddp7969ioqK0o4dOzRhwgTNmzdPEydO1ObNm3XPPfcoISFBU6ZMkSRNmjRJW7Zs0fPPP68hQ4YoJydHhYWFPnXMnTtXzzzzjHr06KGpU6fqjjvu0GeffSZJ+s1vfqPzzz9fS5culdVq1c6dOxUVFdVuzxGAVgjxD3cCOMWUlpYaDoej3q8S33nnncYtt9zi/VXkt99+23vf4cOHjejoaGPFihWGYRjGrbfeaowZM8Zn/gcffNAYOHCgYRiGsWfPHkOSkZmZ2WANdY/x8ccfe9vef/99Q5JRUVFhGIZhdO7c2Xjttddav8IA2h2HpQC0q+zsbFVWVmrMmDGKjY31Tm+88Ya+//57b78RI0Z4r3fr1k0DBgzQ7t27JUm7d+/WqFGjfJY7atQofffdd3K5XNq5c6esVqsuueSSJms599xzvdeTk5MlSQUFBZKkWbNm6a677tKVV16phQsX+tQGoGMj3ABoV263W5L0/vvva+fOnd4pOzvbe95NYywWiyTPOTt11+sYhuG9Hh0d7VctJx5mqlteXX3z5s3Trl27dM011+iTTz7RwIEDtXr1ar+WCyC0CDcA2tXAgQNlt9uVm5urM844w2fq3bu3t9/nn3/uvX706FHt3btXZ511lncZn376qc9yN2/erDPPPFNWq1WDBw+W2+3Whg0bWlXrmWeeqZkzZ+qjjz7SDTfcoFdffbVVywPQPjihGEC76ty5sx544AHNnDlTbrdbF198sYqLi7V582bFxsYqNTVVkjR//nwlJCQoMTFRc+fOVffu3TV+/HhJ0v33368LLrhA//Ef/6GJEydqy5YtWrx4sZYsWSJJ6tu3ryZPnqw77rjDe0Lxjz/+qIKCAk2YMKHZGisqKvTggw/qpptuUlpamn766Sdt27ZNN954Y5s9LwCCKNQn/QA49bjdbmPRokXGgAEDjKioKKNHjx7G2LFjjQ0bNnhP9v3Xv/5lnHPOOYbNZjMuuOACY+fOnT7LePfdd42BAwcaUVFRRp8+fYynn37a5/6Kigpj5syZRnJysmGz2YwzzjjDWL58uWEYx08oPnr0qLd/VlaWIcnIyckxnE6ncfPNNxu9e/c2bDabkZKSYtx7773ek40BdGwWwzjhQDUAhNj69et12WWX6ejRo+rSpUuoywEQhjjnBgAAmArhBgAAmAqHpQAAgKkwcgMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEzl/wcNSzkB0QT9SgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "os.chdir(r\"C:\\Users\\jiwon\\OneDrive\\Desktop\\deep-learning-from-scratch-master\\ch08\")\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(r\"C:\\Users\\jiwon\\OneDrive\\Desktop\\deep-learning-from-scratch-master\\ch08\\deep_convnet_params.pkl\")\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5]\n",
      "b:  [10  4  5]\n",
      "a:  [10  4  5]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([3,4,5])\n",
    "\n",
    "a = b\n",
    "\n",
    "print(a)\n",
    "\n",
    "b[0] = 10\n",
    "\n",
    "print(\"b: \", b)\n",
    "print(\"a: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
