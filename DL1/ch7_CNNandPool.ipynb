{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2, 2)\n",
      "[[[[0.25497366 0.56846035]\n",
      "   [0.04521363 0.76598149]]\n",
      "\n",
      "  [[0.96301921 0.72796039]\n",
      "   [0.93580757 0.95107122]]\n",
      "\n",
      "  [[0.79538642 0.08668937]\n",
      "   [0.21632262 0.71525357]]]\n",
      "\n",
      "\n",
      " [[[0.15275359 0.169905  ]\n",
      "   [0.12487801 0.56480122]]\n",
      "\n",
      "  [[0.63490952 0.84719159]\n",
      "   [0.15482034 0.33841503]]\n",
      "\n",
      "  [[0.26150574 0.87011574]\n",
      "   [0.18465315 0.06186255]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# CNNのデータ\n",
    "x = np.random.rand(2, 3, 2, 2) # 2つのデータ：３チャンネル、２×２\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2)\n",
      "[[[0.25497366 0.56846035]\n",
      "  [0.04521363 0.76598149]]\n",
      "\n",
      " [[0.96301921 0.72796039]\n",
      "  [0.93580757 0.95107122]]\n",
      "\n",
      " [[0.79538642 0.08668937]\n",
      "  [0.21632262 0.71525357]]]\n"
     ]
    }
   ],
   "source": [
    "#0番目のデータを指定\n",
    "print(x[0].shape)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79538642 0.08668937]\n",
      " [0.21632262 0.71525357]]\n"
     ]
    }
   ],
   "source": [
    "# 0番目のデータの2番目チャンネルのデータを指定\n",
    "print(x[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.         0.         0.        ]\n",
      "   [0.         0.25497366 0.56846035 0.        ]\n",
      "   [0.         0.04521363 0.76598149 0.        ]\n",
      "   [0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.        ]\n",
      "   [0.         0.96301921 0.72796039 0.        ]\n",
      "   [0.         0.93580757 0.95107122 0.        ]\n",
      "   [0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.        ]\n",
      "   [0.         0.79538642 0.08668937 0.        ]\n",
      "   [0.         0.21632262 0.71525357 0.        ]\n",
      "   [0.         0.         0.         0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         0.        ]\n",
      "   [0.         0.15275359 0.169905   0.        ]\n",
      "   [0.         0.12487801 0.56480122 0.        ]\n",
      "   [0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.        ]\n",
      "   [0.         0.63490952 0.84719159 0.        ]\n",
      "   [0.         0.15482034 0.33841503 0.        ]\n",
      "   [0.         0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         0.        ]\n",
      "   [0.         0.26150574 0.87011574 0.        ]\n",
      "   [0.         0.18465315 0.06186255 0.        ]\n",
      "   [0.         0.         0.         0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "# パディング\n",
    "x_paded = np.pad(x, [(0,0), (0,0), (1, 1), (1, 1)], 'constant')\n",
    "print(x_paded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_temp(input_data, filter_h, filter_w, stride=1, pad=0): # フィルターの平坦化された要素に合わせて入力も平坦化する\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    print(\"出力の縦out_h:\", out_h)\n",
    "    print(\"出力の横out_w:\", out_w)\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        # フィルター要素の行方向のインデックス\n",
    "    for y in range(filter_h):\n",
    "        # 出力分行方向にスライドした結果の最終地点\n",
    "        y_max = y + stride * out_h\n",
    "\n",
    "        # フィルター要素の列方向のインデックス\n",
    "        for x in range(filter_w):\n",
    "            # 出力分列方向にスライドした結果の最終地点\n",
    "            x_max = x + stride * out_w\n",
    "\n",
    "            # フィルター内の要素(y,x)が(y_max, x_max)までスライに拾っドしている間た入力要素を抽出\n",
    "            # e.g_1 フィルターの左上(0,0)がスライドしていった時拾った入力要素を抽出\n",
    "            # e.g_2 filter_h[0]かつfilter_w[0]ののところに抽出した入力要素を格納\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "            print(\"各データでフィルター要素 y:\" + str(y) + \", x:\" + str(x) + \"が拾った要素:\")\n",
    "            print(col[:, :, y, x, :, :])\n",
    "\n",
    "            # 出力サイズに整形\n",
    "            # それぞれの出力（行）に対してフィルターの要素（列）があるように２次元の配列にする\n",
    "            # e.g_3 フィルター左上(0,0)が抽出した入力要素が各出力に対して１番目（1列目）の要素として格納される（ループで２列目、３列目とフィルターの要素の数だけ続く）\n",
    "            input_data_col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "            # 出力が何個あるか縦に平坦化（さらにデータNの倍数分出力が縦に並ぶ）\n",
    "            print(\"出力の合計（出力の要素数(\", out_h*out_w, \")×データ数(\", N, \")：\", N*out_h*out_w) #e.g. データ２個、出力２×２：８個の出力が縦並び\n",
    "            #フィルターの要素を横に平坦化（さらにチャンネルの倍数分横に並ぶ）\n",
    "            print(\"フィルターの要素数(\", filter_h*filter_w, \")×チャンネル数(\", C, \")：\", C*filter_h*filter_w) # e.g. ３チャンネル3×3のフィルター：９個の要素が横並び\n",
    "            print(\"======フィルター要素 y:\" + str(y) + \", x:\" + str(x) + \"が拾った要素をそれぞれの出力（行）に対して左から詰めていく ==========\")\n",
    "            print(input_data_col)\n",
    "    print(\"それぞれの出力に対して各フィルター要素が拾ったの入力要素が詰め込まれた２次元の形状\", input_data_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力形状： (2, 1, 3, 4)\n",
      "出力の縦out_h: 2\n",
      "出力の横out_w: 3\n",
      "各データでフィルター要素 y:0, x:0が拾った要素:\n",
      "[[[[ 1.  2.  3.]\n",
      "   [11. 22. 33.]]]\n",
      "\n",
      "\n",
      " [[[ 5.  6.  7.]\n",
      "   [55. 66. 77.]]]]\n",
      "出力の合計（出力の要素数( 6 )×データ数( 2 )： 12\n",
      "フィルターの要素数( 4 )×チャンネル数( 1 )： 4\n",
      "======フィルター要素 y:0, x:0が拾った要素をそれぞれの出力に対して左から詰めていく ==========\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.]\n",
      " [11.  0.  0.  0.]\n",
      " [22.  0.  0.  0.]\n",
      " [33.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.]\n",
      " [ 6.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.]\n",
      " [55.  0.  0.  0.]\n",
      " [66.  0.  0.  0.]\n",
      " [77.  0.  0.  0.]]\n",
      "各データでフィルター要素 y:0, x:1が拾った要素:\n",
      "[[[[ 2.  3.  4.]\n",
      "   [22. 33. 44.]]]\n",
      "\n",
      "\n",
      " [[[ 6.  7.  8.]\n",
      "   [66. 77. 88.]]]]\n",
      "出力の合計（出力の要素数( 6 )×データ数( 2 )： 12\n",
      "フィルターの要素数( 4 )×チャンネル数( 1 )： 4\n",
      "======フィルター要素 y:0, x:1が拾った要素をそれぞれの出力に対して左から詰めていく ==========\n",
      "[[ 1.  2.  0.  0.]\n",
      " [ 2.  3.  0.  0.]\n",
      " [ 3.  4.  0.  0.]\n",
      " [11. 22.  0.  0.]\n",
      " [22. 33.  0.  0.]\n",
      " [33. 44.  0.  0.]\n",
      " [ 5.  6.  0.  0.]\n",
      " [ 6.  7.  0.  0.]\n",
      " [ 7.  8.  0.  0.]\n",
      " [55. 66.  0.  0.]\n",
      " [66. 77.  0.  0.]\n",
      " [77. 88.  0.  0.]]\n",
      "各データでフィルター要素 y:1, x:0が拾った要素:\n",
      "[[[[ 11.  22.  33.]\n",
      "   [111. 222. 333.]]]\n",
      "\n",
      "\n",
      " [[[ 55.  66.  77.]\n",
      "   [555. 666. 777.]]]]\n",
      "出力の合計（出力の要素数( 6 )×データ数( 2 )： 12\n",
      "フィルターの要素数( 4 )×チャンネル数( 1 )： 4\n",
      "======フィルター要素 y:1, x:0が拾った要素をそれぞれの出力に対して左から詰めていく ==========\n",
      "[[  1.   2.  11.   0.]\n",
      " [  2.   3.  22.   0.]\n",
      " [  3.   4.  33.   0.]\n",
      " [ 11.  22. 111.   0.]\n",
      " [ 22.  33. 222.   0.]\n",
      " [ 33.  44. 333.   0.]\n",
      " [  5.   6.  55.   0.]\n",
      " [  6.   7.  66.   0.]\n",
      " [  7.   8.  77.   0.]\n",
      " [ 55.  66. 555.   0.]\n",
      " [ 66.  77. 666.   0.]\n",
      " [ 77.  88. 777.   0.]]\n",
      "各データでフィルター要素 y:1, x:1が拾った要素:\n",
      "[[[[ 22.  33.  44.]\n",
      "   [222. 333. 444.]]]\n",
      "\n",
      "\n",
      " [[[ 66.  77.  88.]\n",
      "   [666. 777. 888.]]]]\n",
      "出力の合計（出力の要素数( 6 )×データ数( 2 )： 12\n",
      "フィルターの要素数( 4 )×チャンネル数( 1 )： 4\n",
      "======フィルター要素 y:1, x:1が拾った要素をそれぞれの出力に対して左から詰めていく ==========\n",
      "[[  1.   2.  11.  22.]\n",
      " [  2.   3.  22.  33.]\n",
      " [  3.   4.  33.  44.]\n",
      " [ 11.  22. 111. 222.]\n",
      " [ 22.  33. 222. 333.]\n",
      " [ 33.  44. 333. 444.]\n",
      " [  5.   6.  55.  66.]\n",
      " [  6.   7.  66.  77.]\n",
      " [  7.   8.  77.  88.]\n",
      " [ 55.  66. 555. 666.]\n",
      " [ 66.  77. 666. 777.]\n",
      " [ 77.  88. 777. 888.]]\n",
      "それぞれの出力に対して各フィルター要素が拾ったの入力要素が詰め込まれた２次元の形状 (12, 4)\n"
     ]
    }
   ],
   "source": [
    "# 3×4の1チャンネルを持つ2つのデータを持つ4次元配列を作成\n",
    "x = np.array([\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3, 4],\n",
    "            [11, 22, 33, 44],\n",
    "            [111, 222, 333, 444]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [5, 6, 7, 8],\n",
    "            [55, 66, 77, 88],\n",
    "            [555, 666, 777, 888]\n",
    "        ]\n",
    "    ]\n",
    "])\n",
    "print(\"入力形状：\", x.shape)\n",
    "\n",
    "x_col =  im2col_temp(x, filter_h=2, filter_w=2, stride=1, pad=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平坦化されたフィルターの要素に合わせて入力画像の要素も平坦化する（N*OH*OW(アウトプット数), C*FH*FW(フィルターの要素数)）\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    # print(\"out_h:\", out_h)\n",
    "    # print(\"out_w:\", out_w)\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        print(\"y: \", y )\n",
    "        y_max = y + stride*out_h\n",
    "        print(\"y_max: \", y_max)\n",
    "        for x in range(filter_w):\n",
    "            print(\"x: \", x )\n",
    "            x_max = x + stride*out_w\n",
    "            print(\"x_max: \", x_max)\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "            print(img[:, :, y:y_max:stride, x:x_max:stride])\n",
    "            # print(col[:, :, y, x, :, :])\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "os.chdir(r\"C:\\Users\\jiwon\\OneDrive\\Desktop\\deep-learning-from-scratch-master\\ch03\")\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape # 入力画像（４次元）\n",
    "        FN, C, FH, FW = self.W.shape # 重みフィルター（４次元）\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride) #出力の行数\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride) #出力の列数\n",
    "\n",
    "        # 入力画像（N,C,H,W）を２次元（N*OH*OW(アウトプット数), C*FH*FW(フィルターの要素数)）に変換\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # 入力Xの役割\n",
    "        # 重みフィルター(FN,C,FH,FW)を２次元（FN(フィルターの数), C*FH*FW(フィルターの要素)）に変換して転置(C*FH*FW(フィルターの要素), FN(フィルターの数))\n",
    "        col_W = self.W.reshape(FN, -1).T # 重みWの役割\n",
    "        out = np.dot(col, col_W) + self.b # この時点ではまだ２次元データ（出力の数（N*out_h*out_w）×フィルターの数(FN)）\n",
    "\n",
    "        # ２次元データ（出力の数（N*out_h*out_w）×フィルターの数(FN)）を4次元データに変換\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # (N, FN, OH, OW)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_W, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_W\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        # 出力データに関するサイズを計算\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 入力画像（N,C,H,W）を２次元（N*OH*OW(アウトプット数), C*PH*PW(フィルター(pool)の要素数)）に変換\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # Convだと全チャンネルの入力とフィルタ数値の積和が求めるが、poolではチャンネル別にの最大値を求める\n",
    "        # C*PH*PWだとフィルター要素が拾った入力要素数×チャンネル数で１行に並ぶ　が、poolはチャンネ別にしたいからCを除外したい\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w) # (N*OH*OW*C, PH*PW)\n",
    "\n",
    "        # 各行の最大値\n",
    "        out = np.max(col, axis=1) # (N*OH*OW*C, 1)\n",
    "\n",
    "        # まずreshapeで(N, OH, OW, C)の区切り方に変更（要素の並び順図らない）\n",
    "        # 次にtransposeで(N, C, OH, OW)に入れ替える（要素の並びが出力のための適切な形状）\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "os.chdir(r\"C:\\Users\\jiwon\\OneDrive\\Desktop\\deep-learning-from-scratch-master\\ch03\")\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2*filter_pad) / filter_stride + 1)\n",
    "        # プーリングのフィルターサイズが2×2でストライドが2の場合、プーリング層の出力サイズは常に畳み込み層の出力サイズの半分\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size) #(FN, C, FH, FW)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # 学習途中・終了時のパラメータをファイルに保存し、後で再利用できるようにする\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    # 保存されたパラメータを再利用するためロードする\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 1 train loss:2.2995746974473987\n",
      "No. 2 train loss:2.296878963979722\n",
      "No. 3 train loss:2.292720393291769\n",
      "No. 4 train loss:2.2828924064688505\n",
      "No. 5 train loss:2.2786517795172934\n",
      "No. 6 train loss:2.264715680058616\n",
      "No. 7 train loss:2.249833123722322\n",
      "No. 8 train loss:2.2282298101968885\n",
      "No. 9 train loss:2.220431431703736\n",
      "No. 10 train loss:2.2182116955954454\n",
      "No. 11 train loss:2.1563444958237956\n",
      "No. 12 train loss:2.1476454453349114\n",
      "No. 13 train loss:2.0965100315853094\n",
      "No. 14 train loss:2.0313791560272203\n",
      "No. 15 train loss:1.9725293052167532\n",
      "No. 16 train loss:1.955489912183449\n",
      "No. 17 train loss:1.8371018021100198\n",
      "No. 18 train loss:1.760393312665667\n",
      "No. 19 train loss:1.757633196718424\n",
      "No. 20 train loss:1.5791983242968046\n",
      "No. 21 train loss:1.5029962481869652\n",
      "No. 22 train loss:1.4780608822001784\n",
      "No. 23 train loss:1.4114437532154123\n",
      "No. 24 train loss:1.2855105736875023\n",
      "No. 25 train loss:1.2215973368222008\n",
      "No. 26 train loss:1.0668098635133167\n",
      "No. 27 train loss:1.0887023781753233\n",
      "No. 28 train loss:1.018330741666156\n",
      "No. 29 train loss:1.047373233488583\n",
      "No. 30 train loss:0.8155319132741211\n",
      "No. 31 train loss:0.8056302914639235\n",
      "No. 32 train loss:0.8353730883333089\n",
      "No. 33 train loss:0.7229134295035777\n",
      "No. 34 train loss:0.7868228308896691\n",
      "No. 35 train loss:0.5538983487411026\n",
      "No. 36 train loss:0.6243440998898131\n",
      "No. 37 train loss:0.6260727579970862\n",
      "No. 38 train loss:0.5847238926581066\n",
      "No. 39 train loss:0.46888833915200584\n",
      "No. 40 train loss:0.6543391127228375\n",
      "No. 41 train loss:0.6375413557981318\n",
      "No. 42 train loss:0.5807040732515998\n",
      "No. 43 train loss:0.5449998898584143\n",
      "No. 44 train loss:0.6534880704183027\n",
      "No. 45 train loss:0.6744173749230984\n",
      "No. 46 train loss:0.6192841452526865\n",
      "No. 47 train loss:0.5613353525729314\n",
      "No. 48 train loss:0.6238924327968507\n",
      "No. 49 train loss:0.4976952590664667\n",
      "No. 50 train loss:0.5844993423887099\n",
      "=== epoch:1, train acc:0.799, test acc:0.774 ===\n",
      "No. 51 train loss:0.47421782774749494\n",
      "No. 52 train loss:0.5130975834993741\n",
      "No. 53 train loss:0.4696416639265938\n",
      "No. 54 train loss:0.7374858317415106\n",
      "No. 55 train loss:0.5510920043141876\n",
      "No. 56 train loss:0.6622159521486541\n",
      "No. 57 train loss:0.5154933005322483\n",
      "No. 58 train loss:0.4913334102079612\n",
      "No. 59 train loss:0.5802804268041871\n",
      "No. 60 train loss:0.8053713736842432\n",
      "No. 61 train loss:0.48045980145224293\n",
      "No. 62 train loss:0.40999631985841406\n",
      "No. 63 train loss:0.5697497792844419\n",
      "No. 64 train loss:0.48529595374619094\n",
      "No. 65 train loss:0.42299675217874616\n",
      "No. 66 train loss:0.39369377055771304\n",
      "No. 67 train loss:0.7295342839277739\n",
      "No. 68 train loss:0.44225730049651785\n",
      "No. 69 train loss:0.5178654783492669\n",
      "No. 70 train loss:0.46813973486213234\n",
      "No. 71 train loss:0.3426488621534068\n",
      "No. 72 train loss:0.440956572687721\n",
      "No. 73 train loss:0.3887323661716697\n",
      "No. 74 train loss:0.3981521685453851\n",
      "No. 75 train loss:0.2551103957993171\n",
      "No. 76 train loss:0.6212958397433983\n",
      "No. 77 train loss:0.36964254168107535\n",
      "No. 78 train loss:0.26868363058319295\n",
      "No. 79 train loss:0.5368701892866707\n",
      "No. 80 train loss:0.5083184425289018\n",
      "No. 81 train loss:0.36828264944027034\n",
      "No. 82 train loss:0.4324104822925559\n",
      "No. 83 train loss:0.4793161234951149\n",
      "No. 84 train loss:0.27614248680074205\n",
      "No. 85 train loss:0.4818650296566094\n",
      "No. 86 train loss:0.4118380669211533\n",
      "No. 87 train loss:0.39313489263822654\n",
      "No. 88 train loss:0.4432256058237128\n",
      "No. 89 train loss:0.32921327283551477\n",
      "No. 90 train loss:0.39207615111812444\n",
      "No. 91 train loss:0.2732362409582936\n",
      "No. 92 train loss:0.28788778150214256\n",
      "No. 93 train loss:0.26969551667058\n",
      "No. 94 train loss:0.3798793161998912\n",
      "No. 95 train loss:0.42187657140942414\n",
      "No. 96 train loss:0.21623052671313756\n",
      "No. 97 train loss:0.35392746193917146\n",
      "No. 98 train loss:0.38913935480434647\n",
      "No. 99 train loss:0.3805437711509463\n",
      "No. 100 train loss:0.37550481847579475\n",
      "=== epoch:2, train acc:0.872, test acc:0.865 ===\n",
      "No. 101 train loss:0.4394674941681611\n",
      "No. 102 train loss:0.39712067468154066\n",
      "No. 103 train loss:0.3717144580708136\n",
      "No. 104 train loss:0.26996064293899075\n",
      "No. 105 train loss:0.3438830678048773\n",
      "No. 106 train loss:0.2793346307310945\n",
      "No. 107 train loss:0.44673691283379546\n",
      "No. 108 train loss:0.2556935553113952\n",
      "No. 109 train loss:0.41208666209053957\n",
      "No. 110 train loss:0.38320510635147054\n",
      "No. 111 train loss:0.31531909489027277\n",
      "No. 112 train loss:0.39652420801904986\n",
      "No. 113 train loss:0.262763266786279\n",
      "No. 114 train loss:0.2959196609088634\n",
      "No. 115 train loss:0.32436438878141594\n",
      "No. 116 train loss:0.3869674921273764\n",
      "No. 117 train loss:0.2850190419536291\n",
      "No. 118 train loss:0.4257677180352146\n",
      "No. 119 train loss:0.36255334081562784\n",
      "No. 120 train loss:0.23216957812724762\n",
      "No. 121 train loss:0.42646712030481426\n",
      "No. 122 train loss:0.28547930283225975\n",
      "No. 123 train loss:0.3652695284346703\n",
      "No. 124 train loss:0.19240310466211685\n",
      "No. 125 train loss:0.3614459016055265\n",
      "No. 126 train loss:0.28073198784555264\n",
      "No. 127 train loss:0.2919732107811055\n",
      "No. 128 train loss:0.28601679361100674\n",
      "No. 129 train loss:0.2236442771573924\n",
      "No. 130 train loss:0.3209973058739074\n",
      "No. 131 train loss:0.3668829281667161\n",
      "No. 132 train loss:0.21373057209683843\n",
      "No. 133 train loss:0.22352686919489045\n",
      "No. 134 train loss:0.2420557237933819\n",
      "No. 135 train loss:0.22737599178754583\n",
      "No. 136 train loss:0.32532685680915485\n",
      "No. 137 train loss:0.31203667519683503\n",
      "No. 138 train loss:0.2943559572167124\n",
      "No. 139 train loss:0.20535597301010047\n",
      "No. 140 train loss:0.39992148049443943\n",
      "No. 141 train loss:0.2061102726817246\n",
      "No. 142 train loss:0.37778555225031063\n",
      "No. 143 train loss:0.21983657250932956\n",
      "No. 144 train loss:0.20378743018431777\n",
      "No. 145 train loss:0.34296551327913494\n",
      "No. 146 train loss:0.38447131054405487\n",
      "No. 147 train loss:0.2746280837961046\n",
      "No. 148 train loss:0.2393534849672596\n",
      "No. 149 train loss:0.24361232982547687\n",
      "No. 150 train loss:0.29796832322668926\n",
      "=== epoch:3, train acc:0.9, test acc:0.89 ===\n",
      "No. 151 train loss:0.31913620240664803\n",
      "No. 152 train loss:0.21048462567830156\n",
      "No. 153 train loss:0.33322000714826344\n",
      "No. 154 train loss:0.2884775814087946\n",
      "No. 155 train loss:0.2554987646277985\n",
      "No. 156 train loss:0.3223769989003924\n",
      "No. 157 train loss:0.27982281610058185\n",
      "No. 158 train loss:0.3384215244976865\n",
      "No. 159 train loss:0.37016897259967513\n",
      "No. 160 train loss:0.4198163190017482\n",
      "No. 161 train loss:0.25179471794119024\n",
      "No. 162 train loss:0.28529944325020223\n",
      "No. 163 train loss:0.30499735405437955\n",
      "No. 164 train loss:0.25402148338447456\n",
      "No. 165 train loss:0.17903705505084744\n",
      "No. 166 train loss:0.3003049523414698\n",
      "No. 167 train loss:0.14654094399932183\n",
      "No. 168 train loss:0.2814246170947117\n",
      "No. 169 train loss:0.2922987672060372\n",
      "No. 170 train loss:0.23576167206463455\n",
      "No. 171 train loss:0.2910608017378599\n",
      "No. 172 train loss:0.3709365647743701\n",
      "No. 173 train loss:0.17825473309025797\n",
      "No. 174 train loss:0.15374306608675595\n",
      "No. 175 train loss:0.191821386621126\n",
      "No. 176 train loss:0.25752995759559505\n",
      "No. 177 train loss:0.2190711808910474\n",
      "No. 178 train loss:0.3198730570568754\n",
      "No. 179 train loss:0.20208661280664145\n",
      "No. 180 train loss:0.11990125119294043\n",
      "No. 181 train loss:0.22499756965922488\n",
      "No. 182 train loss:0.3154345918704158\n",
      "No. 183 train loss:0.35506465816014043\n",
      "No. 184 train loss:0.22778953257518178\n",
      "No. 185 train loss:0.20679804225533116\n",
      "No. 186 train loss:0.29415711856857957\n",
      "No. 187 train loss:0.35192541753378853\n",
      "No. 188 train loss:0.35323258224925375\n",
      "No. 189 train loss:0.23336751214443818\n",
      "No. 190 train loss:0.2552166220875526\n",
      "No. 191 train loss:0.276390540827379\n",
      "No. 192 train loss:0.29884157289350993\n",
      "No. 193 train loss:0.3828062112225393\n",
      "No. 194 train loss:0.29198315557070836\n",
      "No. 195 train loss:0.26906349101051164\n",
      "No. 196 train loss:0.3109702451900256\n",
      "No. 197 train loss:0.26608579371617297\n",
      "No. 198 train loss:0.2836966640291506\n",
      "No. 199 train loss:0.33573670041334724\n",
      "No. 200 train loss:0.21843997770047724\n",
      "=== epoch:4, train acc:0.894, test acc:0.881 ===\n",
      "No. 201 train loss:0.33845343031849956\n",
      "No. 202 train loss:0.2670861706958503\n",
      "No. 203 train loss:0.19152475962108198\n",
      "No. 204 train loss:0.3788977666399974\n",
      "No. 205 train loss:0.12505834573992725\n",
      "No. 206 train loss:0.30114571491240183\n",
      "No. 207 train loss:0.11722901586982054\n",
      "No. 208 train loss:0.18309478086180586\n",
      "No. 209 train loss:0.10095246170182204\n",
      "No. 210 train loss:0.22192682545178907\n",
      "No. 211 train loss:0.19311204189571668\n",
      "No. 212 train loss:0.15119400145773032\n",
      "No. 213 train loss:0.15681984577615016\n",
      "No. 214 train loss:0.3877163430420604\n",
      "No. 215 train loss:0.22781566345317444\n",
      "No. 216 train loss:0.16944775798682218\n",
      "No. 217 train loss:0.3819013881049186\n",
      "No. 218 train loss:0.21107170304720388\n",
      "No. 219 train loss:0.24310816431258542\n",
      "No. 220 train loss:0.15533645920741992\n",
      "No. 221 train loss:0.24759640665595556\n",
      "No. 222 train loss:0.16215057376845313\n",
      "No. 223 train loss:0.12768994737218844\n",
      "No. 224 train loss:0.18797165694525184\n",
      "No. 225 train loss:0.23736868797151234\n",
      "No. 226 train loss:0.27597139920887975\n",
      "No. 227 train loss:0.19481538935008344\n",
      "No. 228 train loss:0.28455433596237073\n",
      "No. 229 train loss:0.24115636109450989\n",
      "No. 230 train loss:0.2695027574389798\n",
      "No. 231 train loss:0.38944190441932713\n",
      "No. 232 train loss:0.20655465484373306\n",
      "No. 233 train loss:0.2337088323083094\n",
      "No. 234 train loss:0.17991572137370843\n",
      "No. 235 train loss:0.21979239610628412\n",
      "No. 236 train loss:0.23678050179102608\n",
      "No. 237 train loss:0.24178756872673\n",
      "No. 238 train loss:0.1726013147407174\n",
      "No. 239 train loss:0.4067818152711756\n",
      "No. 240 train loss:0.3570724994369207\n",
      "No. 241 train loss:0.15679761587893964\n",
      "No. 242 train loss:0.23205246420870715\n",
      "No. 243 train loss:0.1509279160367541\n",
      "No. 244 train loss:0.18871591261168763\n",
      "No. 245 train loss:0.1459694305615242\n",
      "No. 246 train loss:0.14899673719174106\n",
      "No. 247 train loss:0.1313243326113679\n",
      "No. 248 train loss:0.25231809128063115\n",
      "No. 249 train loss:0.14558639361631492\n",
      "No. 250 train loss:0.13371919433159907\n",
      "=== epoch:5, train acc:0.923, test acc:0.92 ===\n",
      "No. 251 train loss:0.21499499438361755\n",
      "No. 252 train loss:0.1511477228961428\n",
      "No. 253 train loss:0.12650762117504646\n",
      "No. 254 train loss:0.15639432731706202\n",
      "No. 255 train loss:0.23274204840411347\n",
      "No. 256 train loss:0.17230019061225338\n",
      "No. 257 train loss:0.09902648743459286\n",
      "No. 258 train loss:0.15062526885159122\n",
      "No. 259 train loss:0.20300052103984448\n",
      "No. 260 train loss:0.16470163729514706\n",
      "No. 261 train loss:0.20170432542348515\n",
      "No. 262 train loss:0.1485525842481895\n",
      "No. 263 train loss:0.16447620515675335\n",
      "No. 264 train loss:0.11975233069643658\n",
      "No. 265 train loss:0.34346450543929746\n",
      "No. 266 train loss:0.19568323974321764\n",
      "No. 267 train loss:0.20738381415166302\n",
      "No. 268 train loss:0.11108248247837689\n",
      "No. 269 train loss:0.24213759599166548\n",
      "No. 270 train loss:0.15444745726517145\n",
      "No. 271 train loss:0.18121217703890352\n",
      "No. 272 train loss:0.2184446411926505\n",
      "No. 273 train loss:0.12325033349758112\n",
      "No. 274 train loss:0.17924098697086105\n",
      "No. 275 train loss:0.2707468469888418\n",
      "No. 276 train loss:0.20219465210463863\n",
      "No. 277 train loss:0.30462493423018216\n",
      "No. 278 train loss:0.10257109321346324\n",
      "No. 279 train loss:0.1285877093315754\n",
      "No. 280 train loss:0.16440260222339645\n",
      "No. 281 train loss:0.18680854699158736\n",
      "No. 282 train loss:0.22315873938663766\n",
      "No. 283 train loss:0.15622383870747172\n",
      "No. 284 train loss:0.14813393760642066\n",
      "No. 285 train loss:0.2238799916919122\n",
      "No. 286 train loss:0.21607285568773033\n",
      "No. 287 train loss:0.35210961451423856\n",
      "No. 288 train loss:0.09149680181360172\n",
      "No. 289 train loss:0.1231074477692051\n",
      "No. 290 train loss:0.10756379976905686\n",
      "No. 291 train loss:0.2531894518945277\n",
      "No. 292 train loss:0.28590776240229704\n",
      "No. 293 train loss:0.2239063774468264\n",
      "No. 294 train loss:0.1676118462587863\n",
      "No. 295 train loss:0.14674030459798015\n",
      "No. 296 train loss:0.08534075442191019\n",
      "No. 297 train loss:0.2883771026686423\n",
      "No. 298 train loss:0.19288838449597606\n",
      "No. 299 train loss:0.10405416151122118\n",
      "No. 300 train loss:0.07027620013548014\n",
      "=== epoch:6, train acc:0.937, test acc:0.919 ===\n",
      "No. 301 train loss:0.1031495400526283\n",
      "No. 302 train loss:0.125283551362024\n",
      "No. 303 train loss:0.15583126222026367\n",
      "No. 304 train loss:0.09062548708178321\n",
      "No. 305 train loss:0.15473837602793422\n",
      "No. 306 train loss:0.12824121683558068\n",
      "No. 307 train loss:0.17404223672106706\n",
      "No. 308 train loss:0.15641029096121625\n",
      "No. 309 train loss:0.2514783958566749\n",
      "No. 310 train loss:0.21398336960406542\n",
      "No. 311 train loss:0.1204094826078356\n",
      "No. 312 train loss:0.2021042305818181\n",
      "No. 313 train loss:0.16879710118456548\n",
      "No. 314 train loss:0.15247583203569268\n",
      "No. 315 train loss:0.1085639515050142\n",
      "No. 316 train loss:0.29571479712451776\n",
      "No. 317 train loss:0.10882180150155277\n",
      "No. 318 train loss:0.2699872213838127\n",
      "No. 319 train loss:0.15366379035466937\n",
      "No. 320 train loss:0.1078102583860224\n",
      "No. 321 train loss:0.16069166255539624\n",
      "No. 322 train loss:0.2263378225070743\n",
      "No. 323 train loss:0.13586819729248137\n",
      "No. 324 train loss:0.16750477351082485\n",
      "No. 325 train loss:0.1068027096575504\n",
      "No. 326 train loss:0.22689520265167626\n",
      "No. 327 train loss:0.10805334467326859\n",
      "No. 328 train loss:0.10502410327033845\n",
      "No. 329 train loss:0.11839770744455694\n",
      "No. 330 train loss:0.14242824366979542\n",
      "No. 331 train loss:0.20435165941507855\n",
      "No. 332 train loss:0.31069838471413846\n",
      "No. 333 train loss:0.1486891434534929\n",
      "No. 334 train loss:0.08985630936799058\n",
      "No. 335 train loss:0.1556905051616718\n",
      "No. 336 train loss:0.09747089470532717\n",
      "No. 337 train loss:0.20284199424498664\n",
      "No. 338 train loss:0.04627087374516514\n",
      "No. 339 train loss:0.17401911761818528\n",
      "No. 340 train loss:0.14548507489662163\n",
      "No. 341 train loss:0.1736101610916092\n",
      "No. 342 train loss:0.1473426127556285\n",
      "No. 343 train loss:0.12454937892660373\n",
      "No. 344 train loss:0.1579285841912727\n",
      "No. 345 train loss:0.18530038768048526\n",
      "No. 346 train loss:0.09604180215519388\n",
      "No. 347 train loss:0.10852883333752081\n",
      "No. 348 train loss:0.14166934236841075\n",
      "No. 349 train loss:0.14503160184357256\n",
      "No. 350 train loss:0.09107047413545039\n",
      "=== epoch:7, train acc:0.947, test acc:0.933 ===\n",
      "No. 351 train loss:0.1604855279759865\n",
      "No. 352 train loss:0.11995510760323097\n",
      "No. 353 train loss:0.11623456570115188\n",
      "No. 354 train loss:0.1737431885390179\n",
      "No. 355 train loss:0.09244067033885624\n",
      "No. 356 train loss:0.15492054330406219\n",
      "No. 357 train loss:0.11863177957880683\n",
      "No. 358 train loss:0.12694980996606345\n",
      "No. 359 train loss:0.22471312087909048\n",
      "No. 360 train loss:0.14043783795047232\n",
      "No. 361 train loss:0.17409819683771333\n",
      "No. 362 train loss:0.22388339685516068\n",
      "No. 363 train loss:0.21008153179823047\n",
      "No. 364 train loss:0.18738509408897133\n",
      "No. 365 train loss:0.07898152812900362\n",
      "No. 366 train loss:0.08163735195465115\n",
      "No. 367 train loss:0.1583773825958305\n",
      "No. 368 train loss:0.09380082088333577\n",
      "No. 369 train loss:0.09413563468985904\n",
      "No. 370 train loss:0.09814153792288666\n",
      "No. 371 train loss:0.0617234823392802\n",
      "No. 372 train loss:0.06540093758528932\n",
      "No. 373 train loss:0.13490788633096554\n",
      "No. 374 train loss:0.16532042172054595\n",
      "No. 375 train loss:0.10319853828705902\n",
      "No. 376 train loss:0.10708639404179038\n",
      "No. 377 train loss:0.12042227533518833\n",
      "No. 378 train loss:0.09717731744366062\n",
      "No. 379 train loss:0.1744445822108227\n",
      "No. 380 train loss:0.09305736610228628\n",
      "No. 381 train loss:0.10960661987452357\n",
      "No. 382 train loss:0.1380995097070947\n",
      "No. 383 train loss:0.05289272727950526\n",
      "No. 384 train loss:0.19734632832883606\n",
      "No. 385 train loss:0.07493387168160134\n",
      "No. 386 train loss:0.11620835719701969\n",
      "No. 387 train loss:0.08034192744624909\n",
      "No. 388 train loss:0.1170596612697185\n",
      "No. 389 train loss:0.13629827421033272\n",
      "No. 390 train loss:0.07580089979610258\n",
      "No. 391 train loss:0.16310940950433853\n",
      "No. 392 train loss:0.22888063208996662\n",
      "No. 393 train loss:0.15824289885979648\n",
      "No. 394 train loss:0.11508197949897353\n",
      "No. 395 train loss:0.08478362113807028\n",
      "No. 396 train loss:0.05833925952777456\n",
      "No. 397 train loss:0.23826440854893008\n",
      "No. 398 train loss:0.044349177943654776\n",
      "No. 399 train loss:0.1115463883558685\n",
      "No. 400 train loss:0.11528271943071357\n",
      "=== epoch:8, train acc:0.951, test acc:0.937 ===\n",
      "No. 401 train loss:0.07575790939987477\n",
      "No. 402 train loss:0.25336311353394464\n",
      "No. 403 train loss:0.06336230252041984\n",
      "No. 404 train loss:0.1392631193241896\n",
      "No. 405 train loss:0.05733669566806549\n",
      "No. 406 train loss:0.0810733384713373\n",
      "No. 407 train loss:0.12680862787038166\n",
      "No. 408 train loss:0.19818933088118518\n",
      "No. 409 train loss:0.07445505482456656\n",
      "No. 410 train loss:0.038565409727677705\n",
      "No. 411 train loss:0.09572643774707176\n",
      "No. 412 train loss:0.0764715893601661\n",
      "No. 413 train loss:0.09045387878220092\n",
      "No. 414 train loss:0.03643977483282492\n",
      "No. 415 train loss:0.12848088892800452\n",
      "No. 416 train loss:0.14657430730067456\n",
      "No. 417 train loss:0.14113107638484002\n",
      "No. 418 train loss:0.08139084953416964\n",
      "No. 419 train loss:0.07259560928145588\n",
      "No. 420 train loss:0.10163286698378307\n",
      "No. 421 train loss:0.0876607711961541\n",
      "No. 422 train loss:0.1389527818427686\n",
      "No. 423 train loss:0.2706267191575502\n",
      "No. 424 train loss:0.1801283115246226\n",
      "No. 425 train loss:0.11196241623851767\n",
      "No. 426 train loss:0.11181000652470637\n",
      "No. 427 train loss:0.09102846599312038\n",
      "No. 428 train loss:0.17650123791223554\n",
      "No. 429 train loss:0.1071851941602742\n",
      "No. 430 train loss:0.20419544778549562\n",
      "No. 431 train loss:0.12049390681953807\n",
      "No. 432 train loss:0.11420993998104391\n",
      "No. 433 train loss:0.08900673384208402\n",
      "No. 434 train loss:0.09633329247093105\n",
      "No. 435 train loss:0.08111569236209934\n",
      "No. 436 train loss:0.15495940592377624\n",
      "No. 437 train loss:0.14524558549906788\n",
      "No. 438 train loss:0.0463596538869223\n",
      "No. 439 train loss:0.08551777513781367\n",
      "No. 440 train loss:0.10776511096340174\n",
      "No. 441 train loss:0.060872159077904706\n",
      "No. 442 train loss:0.10306513291393472\n",
      "No. 443 train loss:0.12316953216400953\n",
      "No. 444 train loss:0.0957185696167608\n",
      "No. 445 train loss:0.06237456463051907\n",
      "No. 446 train loss:0.076306137214132\n",
      "No. 447 train loss:0.19980448942076032\n",
      "No. 448 train loss:0.06004226843970171\n",
      "No. 449 train loss:0.058064069134888666\n",
      "No. 450 train loss:0.09358329997714726\n",
      "=== epoch:9, train acc:0.961, test acc:0.932 ===\n",
      "No. 451 train loss:0.1616336590840859\n",
      "No. 452 train loss:0.10698981405623355\n",
      "No. 453 train loss:0.16937635225022557\n",
      "No. 454 train loss:0.1072577685675731\n",
      "No. 455 train loss:0.18701008933440458\n",
      "No. 456 train loss:0.06524377238084139\n",
      "No. 457 train loss:0.04415275489540087\n",
      "No. 458 train loss:0.09250185762833495\n",
      "No. 459 train loss:0.1009640153114589\n",
      "No. 460 train loss:0.149398977663016\n",
      "No. 461 train loss:0.09009001134577689\n",
      "No. 462 train loss:0.11709708155042609\n",
      "No. 463 train loss:0.10589515379262245\n",
      "No. 464 train loss:0.17011849939698376\n",
      "No. 465 train loss:0.17674903342537104\n",
      "No. 466 train loss:0.055718819159623276\n",
      "No. 467 train loss:0.05469507164001402\n",
      "No. 468 train loss:0.10445839336185829\n",
      "No. 469 train loss:0.061971912117653634\n",
      "No. 470 train loss:0.1328366569354575\n",
      "No. 471 train loss:0.1714173418637908\n",
      "No. 472 train loss:0.06173753394634513\n",
      "No. 473 train loss:0.09256392848462519\n",
      "No. 474 train loss:0.050549693532655524\n",
      "No. 475 train loss:0.18547649263046137\n",
      "No. 476 train loss:0.055472517075936215\n",
      "No. 477 train loss:0.06001357482921712\n",
      "No. 478 train loss:0.08330371196790363\n",
      "No. 479 train loss:0.1309078976620784\n",
      "No. 480 train loss:0.17087315910352735\n",
      "No. 481 train loss:0.08920552408944671\n",
      "No. 482 train loss:0.1085773665700583\n",
      "No. 483 train loss:0.07951088259409318\n",
      "No. 484 train loss:0.22594985482567087\n",
      "No. 485 train loss:0.16923827272160621\n",
      "No. 486 train loss:0.12109101880835203\n",
      "No. 487 train loss:0.20959531762922917\n",
      "No. 488 train loss:0.13441466380242711\n",
      "No. 489 train loss:0.12589764303567227\n",
      "No. 490 train loss:0.12465636904649488\n",
      "No. 491 train loss:0.09920669915271853\n",
      "No. 492 train loss:0.14991721654509027\n",
      "No. 493 train loss:0.1376994047941328\n",
      "No. 494 train loss:0.03934994454168024\n",
      "No. 495 train loss:0.15768752593579724\n",
      "No. 496 train loss:0.06502993049548807\n",
      "No. 497 train loss:0.14565699823103442\n",
      "No. 498 train loss:0.06261377321601252\n",
      "No. 499 train loss:0.08399788079261501\n",
      "No. 500 train loss:0.11902338287830623\n",
      "=== epoch:10, train acc:0.961, test acc:0.934 ===\n",
      "No. 501 train loss:0.08706613911913598\n",
      "No. 502 train loss:0.24014330744364742\n",
      "No. 503 train loss:0.07772679686358329\n",
      "No. 504 train loss:0.1022210540040545\n",
      "No. 505 train loss:0.07325268416285294\n",
      "No. 506 train loss:0.12196659146855944\n",
      "No. 507 train loss:0.04892006681851585\n",
      "No. 508 train loss:0.07337928822289252\n",
      "No. 509 train loss:0.06397922165614814\n",
      "No. 510 train loss:0.10721336324105998\n",
      "No. 511 train loss:0.12336508439198872\n",
      "No. 512 train loss:0.05501081206636532\n",
      "No. 513 train loss:0.15099924762908942\n",
      "No. 514 train loss:0.07319940760435442\n",
      "No. 515 train loss:0.03786643395479941\n",
      "No. 516 train loss:0.1228339306954749\n",
      "No. 517 train loss:0.06417751229685527\n",
      "No. 518 train loss:0.11467033034922786\n",
      "No. 519 train loss:0.05579621275968496\n",
      "No. 520 train loss:0.13221146204879988\n",
      "No. 521 train loss:0.09832279801801735\n",
      "No. 522 train loss:0.05181799113158373\n",
      "No. 523 train loss:0.11702534844830315\n",
      "No. 524 train loss:0.11384609608694497\n",
      "No. 525 train loss:0.07723403401661463\n",
      "No. 526 train loss:0.09327979048015572\n",
      "No. 527 train loss:0.03207849611792142\n",
      "No. 528 train loss:0.05442160965373357\n",
      "No. 529 train loss:0.1170801799325996\n",
      "No. 530 train loss:0.05129168212790555\n",
      "No. 531 train loss:0.2595582519378086\n",
      "No. 532 train loss:0.13568082691813754\n",
      "No. 533 train loss:0.11213468565076073\n",
      "No. 534 train loss:0.0985625447578925\n",
      "No. 535 train loss:0.034894103001264595\n",
      "No. 536 train loss:0.0928555769092374\n",
      "No. 537 train loss:0.1121564386806378\n",
      "No. 538 train loss:0.1117549531272626\n",
      "No. 539 train loss:0.11957224409683606\n",
      "No. 540 train loss:0.07935108286033551\n",
      "No. 541 train loss:0.14903447393963495\n",
      "No. 542 train loss:0.059903588345154246\n",
      "No. 543 train loss:0.0344262700127779\n",
      "No. 544 train loss:0.1125736983771065\n",
      "No. 545 train loss:0.05876040915673788\n",
      "No. 546 train loss:0.040571649853585884\n",
      "No. 547 train loss:0.07878919096720922\n",
      "No. 548 train loss:0.07904097606646816\n",
      "No. 549 train loss:0.09078510720278865\n",
      "No. 550 train loss:0.07903299433211691\n",
      "=== epoch:11, train acc:0.973, test acc:0.948 ===\n",
      "No. 551 train loss:0.09766225517256938\n",
      "No. 552 train loss:0.14381405582950493\n",
      "No. 553 train loss:0.06326839894070496\n",
      "No. 554 train loss:0.08121279619705794\n",
      "No. 555 train loss:0.03280161570265125\n",
      "No. 556 train loss:0.09700960821429583\n",
      "No. 557 train loss:0.09147228829442415\n",
      "No. 558 train loss:0.06623348530409662\n",
      "No. 559 train loss:0.0613333236892578\n",
      "No. 560 train loss:0.038150392806292927\n",
      "No. 561 train loss:0.09633968476582006\n",
      "No. 562 train loss:0.05782177920360224\n",
      "No. 563 train loss:0.1255888666926995\n",
      "No. 564 train loss:0.05354643158045016\n",
      "No. 565 train loss:0.02968356573365897\n",
      "No. 566 train loss:0.0786872734618241\n",
      "No. 567 train loss:0.16199566546481978\n",
      "No. 568 train loss:0.0515179327756946\n",
      "No. 569 train loss:0.034674486039272964\n",
      "No. 570 train loss:0.06830151681688362\n",
      "No. 571 train loss:0.08752401346589161\n",
      "No. 572 train loss:0.09162373757994507\n",
      "No. 573 train loss:0.043265829461828734\n",
      "No. 574 train loss:0.06081285746956116\n",
      "No. 575 train loss:0.11979346702395613\n",
      "No. 576 train loss:0.06999928893743596\n",
      "No. 577 train loss:0.053535449407896625\n",
      "No. 578 train loss:0.1123010473288653\n",
      "No. 579 train loss:0.06852473545973888\n",
      "No. 580 train loss:0.08357328546195848\n",
      "No. 581 train loss:0.07444588927570651\n",
      "No. 582 train loss:0.08386461893107057\n",
      "No. 583 train loss:0.08716749629131314\n",
      "No. 584 train loss:0.05948863152513952\n",
      "No. 585 train loss:0.10633994466460525\n",
      "No. 586 train loss:0.09430995541454201\n",
      "No. 587 train loss:0.051506350012928824\n",
      "No. 588 train loss:0.047549925475785425\n",
      "No. 589 train loss:0.058923839186543044\n",
      "No. 590 train loss:0.07379481409790785\n",
      "No. 591 train loss:0.06236392676092465\n",
      "No. 592 train loss:0.07775830357410435\n",
      "No. 593 train loss:0.10003069059759744\n",
      "No. 594 train loss:0.05307187250252287\n",
      "No. 595 train loss:0.029394191240817758\n",
      "No. 596 train loss:0.03736804142993686\n",
      "No. 597 train loss:0.05108119801667068\n",
      "No. 598 train loss:0.05158095764298028\n",
      "No. 599 train loss:0.08794902661375902\n",
      "No. 600 train loss:0.05884857411409279\n",
      "=== epoch:12, train acc:0.974, test acc:0.937 ===\n",
      "No. 601 train loss:0.07663918674630123\n",
      "No. 602 train loss:0.06779277125612893\n",
      "No. 603 train loss:0.059183600114913276\n",
      "No. 604 train loss:0.04755189438582083\n",
      "No. 605 train loss:0.04879630167145556\n",
      "No. 606 train loss:0.046312489957951924\n",
      "No. 607 train loss:0.041543944353796824\n",
      "No. 608 train loss:0.026633779201339677\n",
      "No. 609 train loss:0.08862257367220706\n",
      "No. 610 train loss:0.13709734308933144\n",
      "No. 611 train loss:0.0755858514856298\n",
      "No. 612 train loss:0.026933550629885835\n",
      "No. 613 train loss:0.03260419478030965\n",
      "No. 614 train loss:0.17884248512998613\n",
      "No. 615 train loss:0.058463760262337024\n",
      "No. 616 train loss:0.0513463971463551\n",
      "No. 617 train loss:0.03856993573570368\n",
      "No. 618 train loss:0.07987197323237119\n",
      "No. 619 train loss:0.05698199783321274\n",
      "No. 620 train loss:0.11453162786456997\n",
      "No. 621 train loss:0.027077941061973997\n",
      "No. 622 train loss:0.08627505044903284\n",
      "No. 623 train loss:0.054187655984645415\n",
      "No. 624 train loss:0.02037121126355258\n",
      "No. 625 train loss:0.07975838802166542\n",
      "No. 626 train loss:0.06641261421198047\n",
      "No. 627 train loss:0.016738842716369252\n",
      "No. 628 train loss:0.08212711930670849\n",
      "No. 629 train loss:0.03952527281689348\n",
      "No. 630 train loss:0.11248818951054058\n",
      "No. 631 train loss:0.029101663078356852\n",
      "No. 632 train loss:0.08068732722456919\n",
      "No. 633 train loss:0.0918837457786463\n",
      "No. 634 train loss:0.0782186133527839\n",
      "No. 635 train loss:0.03355706862244678\n",
      "No. 636 train loss:0.13212294769792704\n",
      "No. 637 train loss:0.0770058572925404\n",
      "No. 638 train loss:0.028844361147337527\n",
      "No. 639 train loss:0.06950244102183781\n",
      "No. 640 train loss:0.15780415427457808\n",
      "No. 641 train loss:0.02027486062651435\n",
      "No. 642 train loss:0.07552350910490635\n",
      "No. 643 train loss:0.028915106834981077\n",
      "No. 644 train loss:0.0798189754851816\n",
      "No. 645 train loss:0.07142053539032388\n",
      "No. 646 train loss:0.0347977885881567\n",
      "No. 647 train loss:0.09449132938378695\n",
      "No. 648 train loss:0.035989363298073465\n",
      "No. 649 train loss:0.06393016861611227\n",
      "No. 650 train loss:0.09645629329867074\n",
      "=== epoch:13, train acc:0.971, test acc:0.947 ===\n",
      "No. 651 train loss:0.09015710387807989\n",
      "No. 652 train loss:0.03528363500319085\n",
      "No. 653 train loss:0.05194642474774878\n",
      "No. 654 train loss:0.04323960322785586\n",
      "No. 655 train loss:0.09864087326592133\n",
      "No. 656 train loss:0.03822432103386192\n",
      "No. 657 train loss:0.0563755582218143\n",
      "No. 658 train loss:0.06249292297468619\n",
      "No. 659 train loss:0.058396767853917855\n",
      "No. 660 train loss:0.09635805428143027\n",
      "No. 661 train loss:0.057676840766212545\n",
      "No. 662 train loss:0.03391984762782902\n",
      "No. 663 train loss:0.019302391230605974\n",
      "No. 664 train loss:0.08069620277972088\n",
      "No. 665 train loss:0.05506370010443982\n",
      "No. 666 train loss:0.05624926496355645\n",
      "No. 667 train loss:0.0446339868582914\n",
      "No. 668 train loss:0.09613917320743653\n",
      "No. 669 train loss:0.048926168977448434\n",
      "No. 670 train loss:0.07458440292693216\n",
      "No. 671 train loss:0.05140673313008048\n",
      "No. 672 train loss:0.0833870398630609\n",
      "No. 673 train loss:0.051968993753275974\n",
      "No. 674 train loss:0.05136383772379086\n",
      "No. 675 train loss:0.02931383886883045\n",
      "No. 676 train loss:0.09657329191239196\n",
      "No. 677 train loss:0.0666410025777778\n",
      "No. 678 train loss:0.07452402668603159\n",
      "No. 679 train loss:0.05407116164647063\n",
      "No. 680 train loss:0.030416729742861914\n",
      "No. 681 train loss:0.05144324341607584\n",
      "No. 682 train loss:0.03818671032597126\n",
      "No. 683 train loss:0.031245689608756386\n",
      "No. 684 train loss:0.045067233088157736\n",
      "No. 685 train loss:0.08612567552407588\n",
      "No. 686 train loss:0.07557728292007176\n",
      "No. 687 train loss:0.11333732720023415\n",
      "No. 688 train loss:0.04343963653787723\n",
      "No. 689 train loss:0.039228120040178456\n",
      "No. 690 train loss:0.06418466815991908\n",
      "No. 691 train loss:0.06709530402791374\n",
      "No. 692 train loss:0.01859415992866819\n",
      "No. 693 train loss:0.05302834285789528\n",
      "No. 694 train loss:0.11700293928488127\n",
      "No. 695 train loss:0.03809199212788483\n",
      "No. 696 train loss:0.14802354230353032\n",
      "No. 697 train loss:0.07739717736716062\n",
      "No. 698 train loss:0.06378144007703707\n",
      "No. 699 train loss:0.030137005547284677\n",
      "No. 700 train loss:0.04238748487211461\n",
      "=== epoch:14, train acc:0.974, test acc:0.938 ===\n",
      "No. 701 train loss:0.0707594173724788\n",
      "No. 702 train loss:0.03004453753675626\n",
      "No. 703 train loss:0.029176830868398354\n",
      "No. 704 train loss:0.058285748735084277\n",
      "No. 705 train loss:0.03332397564549953\n",
      "No. 706 train loss:0.030942420462301376\n",
      "No. 707 train loss:0.0349658547987182\n",
      "No. 708 train loss:0.07161896097853768\n",
      "No. 709 train loss:0.05555440042418005\n",
      "No. 710 train loss:0.04034961203300139\n",
      "No. 711 train loss:0.022867905193149903\n",
      "No. 712 train loss:0.053761200481620454\n",
      "No. 713 train loss:0.03547024016327228\n",
      "No. 714 train loss:0.027651231965403197\n",
      "No. 715 train loss:0.045538499798046625\n",
      "No. 716 train loss:0.04942817790097061\n",
      "No. 717 train loss:0.028587701733671565\n",
      "No. 718 train loss:0.019799566336401793\n",
      "No. 719 train loss:0.06180119462463213\n",
      "No. 720 train loss:0.06344843813484173\n",
      "No. 721 train loss:0.021405802623647073\n",
      "No. 722 train loss:0.05996667984675737\n",
      "No. 723 train loss:0.03043776544699\n",
      "No. 724 train loss:0.08618563265522006\n",
      "No. 725 train loss:0.031480307388255885\n",
      "No. 726 train loss:0.04344022484515974\n",
      "No. 727 train loss:0.04727942539984257\n",
      "No. 728 train loss:0.03026929935369558\n",
      "No. 729 train loss:0.049498505591256806\n",
      "No. 730 train loss:0.0579715883599971\n",
      "No. 731 train loss:0.04825930045038703\n",
      "No. 732 train loss:0.016960781953708662\n",
      "No. 733 train loss:0.01929256083910186\n",
      "No. 734 train loss:0.04459372380255667\n",
      "No. 735 train loss:0.0611267025410766\n",
      "No. 736 train loss:0.040442525604119844\n",
      "No. 737 train loss:0.0392414695641874\n",
      "No. 738 train loss:0.020809167260773417\n",
      "No. 739 train loss:0.039158238462506184\n",
      "No. 740 train loss:0.05370983807910705\n",
      "No. 741 train loss:0.030210215216107814\n",
      "No. 742 train loss:0.020110861475052557\n",
      "No. 743 train loss:0.020962511040839273\n",
      "No. 744 train loss:0.03069783029599324\n",
      "No. 745 train loss:0.03247843975696077\n",
      "No. 746 train loss:0.05498555048491913\n",
      "No. 747 train loss:0.04626279338610079\n",
      "No. 748 train loss:0.025796958190347254\n",
      "No. 749 train loss:0.14114644322641476\n",
      "No. 750 train loss:0.17497134932360253\n",
      "=== epoch:15, train acc:0.977, test acc:0.947 ===\n",
      "No. 751 train loss:0.10446219987599671\n",
      "No. 752 train loss:0.030203820538942848\n",
      "No. 753 train loss:0.02297171169814594\n",
      "No. 754 train loss:0.03449231074154328\n",
      "No. 755 train loss:0.04126171862356922\n",
      "No. 756 train loss:0.09529546924228818\n",
      "No. 757 train loss:0.0321146143522035\n",
      "No. 758 train loss:0.02927693698676515\n",
      "No. 759 train loss:0.053881503993665494\n",
      "No. 760 train loss:0.02941479218857988\n",
      "No. 761 train loss:0.05853363344489417\n",
      "No. 762 train loss:0.02120897314135846\n",
      "No. 763 train loss:0.08121354768339603\n",
      "No. 764 train loss:0.020152511246186586\n",
      "No. 765 train loss:0.02723596117223668\n",
      "No. 766 train loss:0.056744387115038764\n",
      "No. 767 train loss:0.03272734738261073\n",
      "No. 768 train loss:0.026430002385059246\n",
      "No. 769 train loss:0.03668839827650834\n",
      "No. 770 train loss:0.025449927099965425\n",
      "No. 771 train loss:0.02148886758245416\n",
      "No. 772 train loss:0.03370812216220581\n",
      "No. 773 train loss:0.07824642962468698\n",
      "No. 774 train loss:0.025356849813978403\n",
      "No. 775 train loss:0.025924390246240926\n",
      "No. 776 train loss:0.015409819744793803\n",
      "No. 777 train loss:0.009984536933194656\n",
      "No. 778 train loss:0.021533236163016535\n",
      "No. 779 train loss:0.027268854977196413\n",
      "No. 780 train loss:0.055296953690712435\n",
      "No. 781 train loss:0.03074074380970008\n",
      "No. 782 train loss:0.025918934201402687\n",
      "No. 783 train loss:0.06326891996196433\n",
      "No. 784 train loss:0.03528780083820283\n",
      "No. 785 train loss:0.01967815366891077\n",
      "No. 786 train loss:0.01484493433624034\n",
      "No. 787 train loss:0.014769600682587557\n",
      "No. 788 train loss:0.04644934562138452\n",
      "No. 789 train loss:0.08677056999820394\n",
      "No. 790 train loss:0.06873784921186243\n",
      "No. 791 train loss:0.013835765526186967\n",
      "No. 792 train loss:0.019709224549187703\n",
      "No. 793 train loss:0.019322205103459587\n",
      "No. 794 train loss:0.04656141206054864\n",
      "No. 795 train loss:0.016707236680345017\n",
      "No. 796 train loss:0.015098476340082627\n",
      "No. 797 train loss:0.04235586628030966\n",
      "No. 798 train loss:0.021385588487014086\n",
      "No. 799 train loss:0.01631539809646484\n",
      "No. 800 train loss:0.06784847736711683\n",
      "=== epoch:16, train acc:0.989, test acc:0.954 ===\n",
      "No. 801 train loss:0.03222640444748388\n",
      "No. 802 train loss:0.06670534456308955\n",
      "No. 803 train loss:0.05996226088931565\n",
      "No. 804 train loss:0.0383771772740972\n",
      "No. 805 train loss:0.08139390375599442\n",
      "No. 806 train loss:0.014501459366954017\n",
      "No. 807 train loss:0.10281377364011376\n",
      "No. 808 train loss:0.02032987935801255\n",
      "No. 809 train loss:0.029172759473040166\n",
      "No. 810 train loss:0.045431698895823505\n",
      "No. 811 train loss:0.04614308826255521\n",
      "No. 812 train loss:0.0202741647770317\n",
      "No. 813 train loss:0.061934506737693996\n",
      "No. 814 train loss:0.0390717474130392\n",
      "No. 815 train loss:0.024858450196027483\n",
      "No. 816 train loss:0.0902688452079699\n",
      "No. 817 train loss:0.0925766759935887\n",
      "No. 818 train loss:0.0407134405753002\n",
      "No. 819 train loss:0.014245806685874956\n",
      "No. 820 train loss:0.06781571957307453\n",
      "No. 821 train loss:0.05648706565278408\n",
      "No. 822 train loss:0.03463058155934286\n",
      "No. 823 train loss:0.030029288596707768\n",
      "No. 824 train loss:0.029781973645476977\n",
      "No. 825 train loss:0.012045069535803681\n",
      "No. 826 train loss:0.03168915780530451\n",
      "No. 827 train loss:0.0682001263655398\n",
      "No. 828 train loss:0.034695015451019565\n",
      "No. 829 train loss:0.04347560058898191\n",
      "No. 830 train loss:0.03050468238102994\n",
      "No. 831 train loss:0.09568627303172661\n",
      "No. 832 train loss:0.026793489691988547\n",
      "No. 833 train loss:0.04739946951407844\n",
      "No. 834 train loss:0.04651699289048222\n",
      "No. 835 train loss:0.04965894792918561\n",
      "No. 836 train loss:0.05796038785861792\n",
      "No. 837 train loss:0.026435526391921163\n",
      "No. 838 train loss:0.03267538402964011\n",
      "No. 839 train loss:0.014017604438649271\n",
      "No. 840 train loss:0.02437382151820176\n",
      "No. 841 train loss:0.04241092007191958\n",
      "No. 842 train loss:0.03993722641316221\n",
      "No. 843 train loss:0.06635346504718168\n",
      "No. 844 train loss:0.027666147178423132\n",
      "No. 845 train loss:0.037207679162898466\n",
      "No. 846 train loss:0.00926584049847654\n",
      "No. 847 train loss:0.052910087400862824\n",
      "No. 848 train loss:0.015953375963100043\n",
      "No. 849 train loss:0.03679240869694958\n",
      "No. 850 train loss:0.02841259552121277\n",
      "=== epoch:17, train acc:0.989, test acc:0.955 ===\n",
      "No. 851 train loss:0.07210278267480735\n",
      "No. 852 train loss:0.024523661904002337\n",
      "No. 853 train loss:0.020085979732882984\n",
      "No. 854 train loss:0.02493707400342882\n",
      "No. 855 train loss:0.04144286078166712\n",
      "No. 856 train loss:0.02472008882540353\n",
      "No. 857 train loss:0.03170781919883106\n",
      "No. 858 train loss:0.027566964873811047\n",
      "No. 859 train loss:0.023283509682779785\n",
      "No. 860 train loss:0.0098707927790853\n",
      "No. 861 train loss:0.010170353647485591\n",
      "No. 862 train loss:0.009403130224588392\n",
      "No. 863 train loss:0.027002404628868253\n",
      "No. 864 train loss:0.024513534755114693\n",
      "No. 865 train loss:0.018151806896360326\n",
      "No. 866 train loss:0.023661414513753053\n",
      "No. 867 train loss:0.04190719355269619\n",
      "No. 868 train loss:0.022200884661757\n",
      "No. 869 train loss:0.027027416394827867\n",
      "No. 870 train loss:0.03040504466937828\n",
      "No. 871 train loss:0.02391755378220109\n",
      "No. 872 train loss:0.013017173682921856\n",
      "No. 873 train loss:0.022822340753046275\n",
      "No. 874 train loss:0.013396330083863291\n",
      "No. 875 train loss:0.0316715271215876\n",
      "No. 876 train loss:0.07682225700697974\n",
      "No. 877 train loss:0.01400659837936444\n",
      "No. 878 train loss:0.03969361730479089\n",
      "No. 879 train loss:0.021206627226769727\n",
      "No. 880 train loss:0.010732585423855679\n",
      "No. 881 train loss:0.006893929766765303\n",
      "No. 882 train loss:0.05459145221062252\n",
      "No. 883 train loss:0.05572296232547209\n",
      "No. 884 train loss:0.03884134032471494\n",
      "No. 885 train loss:0.012254908964069324\n",
      "No. 886 train loss:0.014110081732400612\n",
      "No. 887 train loss:0.026647885427991885\n",
      "No. 888 train loss:0.010679974450571148\n",
      "No. 889 train loss:0.038218457320136755\n",
      "No. 890 train loss:0.02069191586768106\n",
      "No. 891 train loss:0.05516258781507089\n",
      "No. 892 train loss:0.011320763660165793\n",
      "No. 893 train loss:0.017977565297979736\n",
      "No. 894 train loss:0.0242764498982982\n",
      "No. 895 train loss:0.01646619586107759\n",
      "No. 896 train loss:0.026617924651815898\n",
      "No. 897 train loss:0.01945235416741202\n",
      "No. 898 train loss:0.02679667632751913\n",
      "No. 899 train loss:0.022467044289241556\n",
      "No. 900 train loss:0.01446691994216591\n",
      "=== epoch:18, train acc:0.991, test acc:0.952 ===\n",
      "No. 901 train loss:0.02340998403886907\n",
      "No. 902 train loss:0.038765202623807185\n",
      "No. 903 train loss:0.04397148370040286\n",
      "No. 904 train loss:0.02199502353200868\n",
      "No. 905 train loss:0.0061329183110634036\n",
      "No. 906 train loss:0.07132522369139663\n",
      "No. 907 train loss:0.03584943852406077\n",
      "No. 908 train loss:0.04040523265335454\n",
      "No. 909 train loss:0.007287210543922837\n",
      "No. 910 train loss:0.03128868651106334\n",
      "No. 911 train loss:0.01779462650849233\n",
      "No. 912 train loss:0.011450005142417967\n",
      "No. 913 train loss:0.020487561011929306\n",
      "No. 914 train loss:0.031845550744026105\n",
      "No. 915 train loss:0.020992161752317866\n",
      "No. 916 train loss:0.021066424933882916\n",
      "No. 917 train loss:0.036538318808514283\n",
      "No. 918 train loss:0.014667198676540349\n",
      "No. 919 train loss:0.008502809787402352\n",
      "No. 920 train loss:0.015339490467236223\n",
      "No. 921 train loss:0.007683339988793167\n",
      "No. 922 train loss:0.017043455016444945\n",
      "No. 923 train loss:0.016004840713697163\n",
      "No. 924 train loss:0.010260307578292186\n",
      "No. 925 train loss:0.01232096167991805\n",
      "No. 926 train loss:0.030037595334122296\n",
      "No. 927 train loss:0.01880650507489563\n",
      "No. 928 train loss:0.04847471356665623\n",
      "No. 929 train loss:0.02111970827362422\n",
      "No. 930 train loss:0.03435513324589207\n",
      "No. 931 train loss:0.053312183283736554\n",
      "No. 932 train loss:0.01192630911302119\n",
      "No. 933 train loss:0.030958615694635955\n",
      "No. 934 train loss:0.05051153682782308\n",
      "No. 935 train loss:0.014647914067746935\n",
      "No. 936 train loss:0.043875108674954325\n",
      "No. 937 train loss:0.037870241886850974\n",
      "No. 938 train loss:0.027220960236109\n",
      "No. 939 train loss:0.02967049630527792\n",
      "No. 940 train loss:0.012005048627729175\n",
      "No. 941 train loss:0.006343398972940213\n",
      "No. 942 train loss:0.031973365968995984\n",
      "No. 943 train loss:0.0242594706568841\n",
      "No. 944 train loss:0.07286503102502792\n",
      "No. 945 train loss:0.017944330862986163\n",
      "No. 946 train loss:0.016029463249693097\n",
      "No. 947 train loss:0.057742771525543225\n",
      "No. 948 train loss:0.013260899571761211\n",
      "No. 949 train loss:0.013007776817445632\n",
      "No. 950 train loss:0.06318582561838902\n",
      "=== epoch:19, train acc:0.991, test acc:0.949 ===\n",
      "No. 951 train loss:0.004153925471381063\n",
      "No. 952 train loss:0.03421323794447049\n",
      "No. 953 train loss:0.014208903337930812\n",
      "No. 954 train loss:0.03512596676156732\n",
      "No. 955 train loss:0.023201065482020514\n",
      "No. 956 train loss:0.010239432978650513\n",
      "No. 957 train loss:0.029639905035791846\n",
      "No. 958 train loss:0.035278363106309926\n",
      "No. 959 train loss:0.026650288761157697\n",
      "No. 960 train loss:0.01292101481041004\n",
      "No. 961 train loss:0.057356446058406535\n",
      "No. 962 train loss:0.007336726539863999\n",
      "No. 963 train loss:0.049995641254442065\n",
      "No. 964 train loss:0.026517052961144615\n",
      "No. 965 train loss:0.011141639974603976\n",
      "No. 966 train loss:0.02889144996934225\n",
      "No. 967 train loss:0.015249289665178191\n",
      "No. 968 train loss:0.01117478814205751\n",
      "No. 969 train loss:0.03406461168965088\n",
      "No. 970 train loss:0.051560914858743014\n",
      "No. 971 train loss:0.03882824093363585\n",
      "No. 972 train loss:0.041482595576236896\n",
      "No. 973 train loss:0.0070913880727309486\n",
      "No. 974 train loss:0.02636364371991028\n",
      "No. 975 train loss:0.03990634923280648\n",
      "No. 976 train loss:0.03177118218812791\n",
      "No. 977 train loss:0.041047039928994854\n",
      "No. 978 train loss:0.021781090143434687\n",
      "No. 979 train loss:0.0209984759936402\n",
      "No. 980 train loss:0.024009165390246077\n",
      "No. 981 train loss:0.011998532095019577\n",
      "No. 982 train loss:0.040825217224401474\n",
      "No. 983 train loss:0.019976629431785898\n",
      "No. 984 train loss:0.011253887956523372\n",
      "No. 985 train loss:0.06197354072190928\n",
      "No. 986 train loss:0.0271926301334019\n",
      "No. 987 train loss:0.006935629987821665\n",
      "No. 988 train loss:0.059030404379902146\n",
      "No. 989 train loss:0.028884614108965305\n",
      "No. 990 train loss:0.009523234867626799\n",
      "No. 991 train loss:0.017836097698929396\n",
      "No. 992 train loss:0.028703838045845583\n",
      "No. 993 train loss:0.024966240235214397\n",
      "No. 994 train loss:0.03282051890795922\n",
      "No. 995 train loss:0.01632222507548184\n",
      "No. 996 train loss:0.015307160054090394\n",
      "No. 997 train loss:0.018081856586442666\n",
      "No. 998 train loss:0.04443751911952367\n",
      "No. 999 train loss:0.040645999947538126\n",
      "No. 1000 train loss:0.022612163108172892\n",
      "=== epoch:20, train acc:0.993, test acc:0.958 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28),\n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
